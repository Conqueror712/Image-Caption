{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc24b0cd",
   "metadata": {},
   "source": [
    "## 结题报告——Image Caption\n",
    "\n",
    "> 2023 秋季北京邮电大学深度学习与神经网络课程设计\n",
    "\n",
    "## 一、任务说明\n",
    "- 成员介绍：\n",
    "    巩羽飞（负责人）、黄成梓（组员）\n",
    "\n",
    "- 任务分工：\n",
    "  - **巩羽飞**：实现了`网格/区域表示、Transformer编码器+Transformer解码器`的模型结构并成功在DeepFashion-MultiModal 数据集上进行训练；实现了METEOR、ROUGE-L评测标准；实现了基于强化学习的损失函数；微调多模态模型BLIP并成功测试其性能；利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建全新的服饰图像描述数据集。\n",
    "  - **黄成梓**：实现了`网格/区域表示、自注意力+注意力`的模型结构并成功在DeepFashion-MultiModal 数据集上进行训练；实现了CIDEr-D评测标准；实现了基于强化学习的损失函数，直接优化CIDEr-D评测标准，使模型的效果得到显著的提升；利用训练的服饰图像描述模型为真实背景的服饰图像数据集增加服饰描述，构建全新的服饰图像描述数据集；在新数据集上重新训练服饰图像描述模型。\n",
    "- 任务完成情况：\n",
    "    小组成员全部完成各自的分工，**完成了所有的必选任务和可选任务**。具体的代码放在了GitHub中：`https://github.com/Conqueror712/Image-Caption`\n",
    "\n",
    "## 二、实验数据\n",
    "\n",
    "### 2.1 原始数据\n",
    "\n",
    "我们使用了 DeepFashion-MultiModal 数据集中 image 和 textual descriptions 的数据，其中 80% 的数据作为模型的训练集，20% 作为模型的测试集。数据集的 Github Repo 如下：\n",
    "\n",
    ">  https://github.com/yumingj/DeepFashion-MultiModal\n",
    "\n",
    "由于数据对应的 json 文件已经提前划分好，但是 images 文件夹仍然是混合在一起的，所以我们编写了一个 Python 脚本用于将 images 划分为 train_images 和 test_images，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbcefd",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# 读取json文件并转换为字典\n",
    "with open('../../data/test_captions.json', 'r') as f:\n",
    "    test_captions = json.load(f)\n",
    "\n",
    "with open('../../data/train_captions.json', 'r') as f:\n",
    "    train_captions = json.load(f)\n",
    "\n",
    "# 指定源目录和目标目录\n",
    "source_directory = '../../data/images'\n",
    "train_directory = '../../data/train_images'\n",
    "test_directory = '../../data/test_images'\n",
    "\n",
    "# 确保目标目录存在\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "# 将训练集图片复制到目标目录\n",
    "for image in train_captions:\n",
    "    shutil.copy(os.path.join(source_directory, image), train_directory)\n",
    "\n",
    "# 将测试集图片复制到目标目录\n",
    "for image in test_captions:\n",
    "    shutil.copy(os.path.join(source_directory, image), test_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d61de",
   "metadata": {},
   "source": [
    "另外，我们还发现图像的关键点信息并没有在 json 文件中显示，而是在图像的文件名中，所以我们**通过正则表达式，提取了每张图像的关键点信息**，并更新了 json 文件，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4b380",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个函数来解析文件名\n",
    "def parse_filename(filename):\n",
    "    # 使用正则表达式匹配文件名\n",
    "    pattern = r'^(?P<gender>\\w+)-(?P<clothing>[\\w_]+)-id_(?P<id>\\d+)-(?P<group>\\d+)_(\\d+_(?P<body>\\w+))\\.jpg$'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 定义一个函数来处理目录中的所有文件\n",
    "def process_directory(directory):\n",
    "    # 创建一个字典来存储结果\n",
    "    results = {}\n",
    "    # 遍历目录中的所有文件\n",
    "    for filename in os.listdir(directory):\n",
    "        # 解析文件名\n",
    "        info = parse_filename(filename)\n",
    "        if info:\n",
    "            # 将解析的信息与文件名关联起来\n",
    "            results[filename] = info\n",
    "    return results\n",
    "\n",
    "# 使用函数处理目录\n",
    "directory = '../../data/images'\n",
    "results = process_directory(directory)\n",
    "\n",
    "# 将结果保存到json文件中\n",
    "with open('../../data/label.json', 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ac1e9",
   "metadata": {},
   "source": [
    "### 2.2 背景描述增量数据\n",
    "\n",
    "我们使用老师在群里提供的数据。\n",
    "\n",
    "> https://pan.baidu.com/s/1qN3EEUNXh4nUcNZMCoT9Fg?pwd=rnfw (rnfw)\n",
    "\n",
    "同样地，我们将其重命名并划分为 train_images 和 test_images，比例为 9:1。\n",
    "\n",
    "![image](../doc/img/Ex_data.png)\n",
    "\n",
    "图片重命名的代码片段如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e621e",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_images(folder_path):\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"文件夹 '{folder_path}' 不存在。\")\n",
    "        return\n",
    "\n",
    "    # 获取文件夹下所有文件\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # 迭代处理每个文件\n",
    "    for index, file_name in enumerate(files):\n",
    "        old_path = os.path.join(folder_path, file_name)\n",
    "        new_name = f\"train_{index + 1}.jpg\"\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"重命名文件: {file_name} -> {new_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8189f",
   "metadata": {},
   "source": [
    "## 三、实验环境\n",
    "\n",
    "- Ubuntu 20.04 / Ubuntu 22.04 / Windows 11\n",
    "- NVIDIA GPU and NVIDIA CUDA Driver\n",
    "- CUDA 12.2 / CUDA 11.8\n",
    "- Python 3.10\n",
    "\n",
    "具体的第三方库依赖请见 `requirements.txt`\n",
    "\n",
    "## 四、模型选择\n",
    "\n",
    "首先，作为共性特点，我们先来介绍一下基于编解码器的方法：\n",
    "\n",
    "随着深度学习技术的不断发展，神经网络在计算机视觉和自然语言处理领域得到了广泛应用。受机器翻译领域中编解码器模型的启发，图像描述可以通过端到端的学习方法直接实现图像和描述句子之间的映射，将图像描述过程转化成为图像到描述的\"翻译\"过程。**基于深度学习的图像描述生成方法大多采用以 CNN-RNN 为基本模型的编解码器框架，CNN 决定了整个模型的图像识别能力，其最后的隐藏层的输出被用作解码器的输入，RNN 是用来读取编码后的图像并生成文本描述的网络模型**，下图是一个简单递归神经网络 RNN 和多模态递归神经网络 m-RNN 架构的示意图：\n",
    "\n",
    "![image](../doc/img/01.png)\n",
    "\n",
    "之所以是 CNN 决定了整个模型的图像识别能力，RNN 被用来读取编码后的图像并生成文本描述的网络模型，是因为计算机视觉问题有很强的局部特征，即一个像素和它周围的旁边几个像素有很强的关联性，但是离他非常远的像素之间的关联性就比较弱，所以只需要像 CNN 一样在局部做连接，而不需要像全连接网络一样每一层之间都是全连接的，从而大大降低了权重的数量。而 RNN 通过使用带自反馈的神经元，也就是隐藏状态，能够处理任意长度的序列数据，可以有效保存序列数据的历史信息。\n",
    "\n",
    "![image](../doc/img/CNN.png)\n",
    "\n",
    "![image](../doc/img/RNN.png)\n",
    "\n",
    "### 4.1 网格/区域表示、自注意力+注意力的模型结构：\n",
    "\n",
    "近年来，注意力机制被广泛应用于计算机视觉领域，其本质是为了解决编解码器在处理固定长度向量时的局限性。注意力机制并不是将输入序列编码成一个固定向量，而是通过增加一个上下文向量来对每个时间步的输入进行解码，以增强图像区域和单词的相关性，从而获取更多的图像语义细节，下图是一个学习单词 / 图像对齐过程的示意图：\n",
    "\n",
    "![image](../doc/img/02.png)\n",
    "\n",
    "#### 4.1.1 参数配置：\n",
    "\n",
    "我在`configurations.py` 文件中定义了 `Config` 类作为项目的配置中心来集中管理项目中使用的所有配置参数。这些参数通常包括文件路径、模型参数、数据处理选项、训练设置和图像处理参数等。通过这种方式，可以在不修改代码的情况下调整项目的行为。以下是 `Config` 类中定义的配置参数及其作用：\n",
    "\n",
    "1. **数据路径**：\n",
    "   - `data_path`：主数据目录路径。\n",
    "   - `images_path`：存储图像的路径。\n",
    "   - `train_captions_path`：训练集的文本描述文件路径。\n",
    "   - `test_captions_path`：测试集的文本描述文件路径。\n",
    "   - `output_folder`：用于存储词汇表和处理后数据的输出文件夹路径。\n",
    "\n",
    "2. **模型参数**：\n",
    "   - `embed_size`：嵌入向量的维度。\n",
    "   - `vocab_size`：词汇表的大小。\n",
    "   - `num_layers`：定义循环神经网络中的层数。\n",
    "   - `num_heads`：自注意力机制中头的数量。\n",
    "   - `dropout`：在模型中使用的 Dropout 比率。\n",
    "   - `hidden_size`：隐藏层的维度。\n",
    "   - `image_code_dim`：图像编码的维度。\n",
    "   - `word_dim`：词嵌入的维度。\n",
    "   - `attention_dim`：注意力机制的隐藏层维度。\n",
    "\n",
    "3. **数据处理参数**：\n",
    "   - `min_word_count`：词汇表中词的最小出现次数，用于筛选词汇。\n",
    "   - `max_len`：假设的描述的最大长度。\n",
    "\n",
    "4. **训练参数**：\n",
    "   - `batch_size`：每个批次的大小。\n",
    "   - `learning_rate`：学习率。\n",
    "   - `num_epochs`：训练的总轮次数。\n",
    "   - `workers`：加载数据时使用的工作线程数。\n",
    "   - `encoder_learning_rate`：编码器的学习率。\n",
    "   - `decoder_learning_rate`：解码器的学习率。\n",
    "   - `lr_update`：学习率更新频率。\n",
    "\n",
    "5. **图像预处理参数**：\n",
    "   - `image_size`：图像缩放后的大小。\n",
    "   - `crop_size`：从缩放后的图像中裁剪出的大小。\n",
    "\n",
    "6. **其他配置**：\n",
    "   - `device`：设置运行计算的设备，如果 CUDA 可用则使用 GPU，否则使用 CPU。\n",
    "\n",
    "#### 4.1.2 数据预处理：\n",
    "\n",
    "我定义了一个python文件`datasets.py`，其定义一个完整的流程，用于创建和处理一个基于图像和文本描述的`DeepFashion-MultiModal` 数据集\n",
    "\n",
    "1. `create_dataset` 函数：用于处理原始文本描述，创建一个词汇表，并将文本转换为对应的词索引向量。它首先读取训练和测试数据集中的文本描述，然后统计词频以创建词汇表，并移除低频词。之后，它定义了一个内部函数 `encode_captions`，这个函数负责将每条文本描述转换为一个固定长度的词索引序列，包括特殊标记 <start>, <end>, <pad>, 和 <unk>。转换完成后，函数将这些数据保存为 json 文件，以便后续处理。关键代码展示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7114c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "def create_dataset(max_len=64):\n",
    "    \"\"\"\n",
    "    整理数据集，构建词汇表，并将文本描述转换为词索引向量。\n",
    "    使用configuration.py文件中定义的配置信息。\n",
    "    \"\"\"\n",
    "    # 使用config中定义的路径\n",
    "    image_folder = config.images_path\n",
    "    train_captions_path = config.train_captions_path\n",
    "    test_captions_path = config.test_captions_path\n",
    "    output_folder = config.output_folder\n",
    "\n",
    "    # 读取训练图像描述\n",
    "    with open(train_captions_path, 'r') as f:\n",
    "        train_captions_data = json.load(f)\n",
    "\n",
    "    # 读取测试图像描述\n",
    "    with open(test_captions_path, 'r') as f:\n",
    "        test_captions_data = json.load(f)\n",
    "\n",
    "    # 统计训练集的文本描述的词频\n",
    "    vocab = Counter()\n",
    "    for caption in train_captions_data.values():\n",
    "        vocab.update(caption.lower().split())\n",
    "\n",
    "    # 移除其中的低频词\n",
    "    vocab = {word for word, count in vocab.items() if count >= config.min_word_count}\n",
    "\n",
    "    # 构建词典\n",
    "    word_to_idx = {word: idx + 4 for idx, word in enumerate(vocab)}\n",
    "    word_to_idx['<pad>'] = 0\n",
    "    word_to_idx['<start>'] = 1\n",
    "    word_to_idx['<end>'] = 2\n",
    "    word_to_idx['<unk>'] = 3\n",
    "\n",
    "    # 一个函数来转换描述为词索引向量，并进行填充\n",
    "    def encode_captions(captions_data, word_to_idx, max_len):\n",
    "        encoded_captions = {}\n",
    "        caplens = {}\n",
    "        for img_id, caption in captions_data.items():\n",
    "            words = caption.lower().split()\n",
    "            encoded_caption = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "            caplen = len(encoded_caption) + 2  # 加2是因为还要加上<start>和<end>\n",
    "            encoded_caption = [word_to_idx['<start>']] + encoded_caption + [word_to_idx['<end>']]\n",
    "            encoded_caption += [word_to_idx['<pad>']] * (max_len - len(encoded_caption))\n",
    "            encoded_captions[img_id] = encoded_caption[:max_len]\n",
    "            caplens[img_id] = caplen if caplen <= max_len else max_len\n",
    "        return encoded_captions, caplens\n",
    "\n",
    "    # 对训练集描述进行编码\n",
    "    encoded_captions_train, caplens_train = encode_captions(train_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 对测试集描述进行编码\n",
    "    encoded_captions_test, caplens_test = encode_captions(test_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 存储词典和编码后的描述\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as f:\n",
    "        json.dump(word_to_idx, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_train.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_test.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_test, f)\n",
    "\n",
    "    # 存储图像路径\n",
    "    image_paths_train = {img_id: os.path.join(image_folder, img_id) for img_id in train_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_train.json'), 'w') as f:\n",
    "        json.dump(image_paths_train, f)\n",
    "\n",
    "    image_paths_test = {img_id: os.path.join(image_folder, img_id) for img_id in test_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_test.json'), 'w') as f:\n",
    "        json.dump(image_paths_test, f)\n",
    "\n",
    "    # 存储caplens\n",
    "    with open(os.path.join(output_folder, 'caplens_train.json'), 'w') as f:\n",
    "        json.dump(caplens_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'caplens_test.json'), 'w') as f:\n",
    "        json.dump(caplens_test, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0106e",
   "metadata": {},
   "source": [
    "2. `ImageTextDataset` 类：继承自 `torch.utils.data.Dataset`，这个类是一个 PyTorch 的自定义数据集，用于加载图像和对应的已编码文本描述。它重写了 `__getitem__` 方法，用于获取索引对应的数据点（图像和文本描述），并将图像通过转换处理成统一的格式；重写了 `__len__` 方法，返回数据集的大小，关键代码展示如下：\n",
    "    ```Python\n",
    "    class ImageTextDataset(Dataset):\n",
    "        \"\"\"\n",
    "        PyTorch数据集类，用于加载和处理图像-文本数据。\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, image_paths_file, captions_file, caplens_file, transform=None):\n",
    "    \"\"\"\n",
    "    初始化数据集类。\n",
    "    参数:\n",
    "        image_paths_file: 包含图像路径的json文件路径。\n",
    "        captions_file: 包含编码后文本描述的json文件路径。\n",
    "        transform: 应用于图像的预处理转换。\n",
    "    \"\"\"\n",
    "    # 载入图像路径和文本描述以及caplens\n",
    "    with open(image_paths_file, 'r') as f:\n",
    "        self.image_paths = json.load(f)\n",
    "    \n",
    "    with open(captions_file, 'r') as f:\n",
    "        self.captions = json.load(f)\n",
    "    \n",
    "    with open(caplens_file, 'r') as f:\n",
    "        self.caplens = json.load(f)\n",
    "    \n",
    "    # 设置图像预处理方法\n",
    "    self.transform = transform or transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    获取单个数据点。\n",
    "    参数:\n",
    "        index: 数据点的索引。\n",
    "    返回:\n",
    "        一个包含图像和对应文本描述的元组。\n",
    "    \"\"\"\n",
    "    # 获取图像路径和文本描述以及caplen\n",
    "    image_id = list(self.image_paths.keys())[index]\n",
    "    image_path = self.image_paths[image_id]\n",
    "    caption = self.captions[image_id]\n",
    "    caplen = self.caplens[image_id]\n",
    "    \n",
    "    # 加载图像并应用预处理\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if self.transform is not None:\n",
    "        image = self.transform(image)\n",
    "    \n",
    "    # 将文本描述转换为张量\n",
    "    caption_tensor = torch.tensor(caption, dtype=torch.long)\n",
    "    \n",
    "    return image, caption_tensor, caplen\n",
    "    \n",
    "def __len__(self):\n",
    "    \"\"\"\n",
    "    数据集中的数据点总数。\n",
    "    \"\"\"\n",
    "    return len(self.image_paths)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da126963",
   "metadata": {},
   "source": [
    "3. `create_dataloaders` 函数：使用 `ImageTextDataset` 类来创建PyTorch的 `DataLoader`，它提供了一个可迭代的数据加载器，用于在训练和测试时批量加载数据，并可选地对数据进行打乱和多进程加载，关键代码展示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 创建训练集和测试集的 DataLoader\n",
    "def create_dataloaders(config):\n",
    "    \"\"\"\n",
    "    创建训练集和测试集的 DataLoader。\n",
    "\n",
    "    参数:\n",
    "        batch_size: 每个批次的大小。\n",
    "        num_workers: 加载数据时使用的进程数。\n",
    "        shuffle_train: 是否打乱训练数据。\n",
    "\n",
    "    返回:\n",
    "        train_loader: 训练数据的 DataLoader。\n",
    "        test_loader: 测试数据的 DataLoader。\n",
    "    \"\"\"\n",
    "    # 图像预处理转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 加载数据时使用的进程数\n",
    "    num_workers = 0\n",
    "\n",
    "    # 创建数据集对象\n",
    "    train_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_train.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_train.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_train.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_test.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_test.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_test.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # 创建 DataLoader 对象\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,  # 通常测试集不需要打乱\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be69289",
   "metadata": {},
   "source": [
    "4. 我还定义了一个 `datasets_pretrain_demo.py` 文件来验证数据预处理过程是否正确。它通过以下步骤实现这一目标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c74456",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 读取词汇表和编码后的描述：加载之前生成的词汇表 `vocab.json`，编码后的训练集描述 `encoded_captions_train.json`，以及训练图像的路径 `image_paths_train.json`。\n",
    "- 索引到单词的转换： 创建从词索引到单词的反向映射，用于将编码后的描述转换回文本形式。\n",
    "- 选择并展示图像： 从图像路径列表中选择第一个图像 ID，并加载对应的图像。\n",
    "- 展示图像：使用 matplotlib 展示图像，并关闭坐标轴。\n",
    "- 打印文本描述：将编码后的描述（词索引列表）转换回单词形式，并打印出来，以验证编码和图像加载的正确性。\n",
    "\n",
    "```Python\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_path = '../data/output/vocab.json'\n",
    "encoded_captions_path = '../data/output/encoded_captions_train.json'\n",
    "image_paths_path = '../data/output/image_paths_train.json'\n",
    "\n",
    "# 读取词典、编码后的描述和图像路径\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open(encoded_captions_path, 'r') as f:\n",
    "    encoded_captions = json.load(f)\n",
    "\n",
    "with open(image_paths_path, 'r') as f:\n",
    "    image_paths = json.load(f)\n",
    "\n",
    "# 将索引转换回单词\n",
    "vocab_idx2word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# 选择要展示的图片ID，这里以第一个ID为例\n",
    "first_img_id = list(image_paths.keys())[0]\n",
    "content_img = Image.open(image_paths[first_img_id])\n",
    "\n",
    "# 展示图片和对应的描述\n",
    "plt.imshow(content_img)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "# 打印对应的文本描述，确保字典中的键是整数，直接使用整数索引\n",
    "caption = ' '.join([vocab_idx2word[word_idx] for word_idx in encoded_captions[first_img_id]])\n",
    "print(caption)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e126c",
   "metadata": {},
   "source": [
    "#### 4.1.3 模型定义\n",
    "\n",
    "我的 `models.py` 文件定义了用于服饰图像描述任务的神经网络模型，包括图像编码器、注意力机制、文本解码器、整体模型框架和损失函数。以下是代码中各个部分的详细作用：\n",
    "\n",
    "1. **自注意力机制** `SelfAttention` 类：定义了一个利用 `nn.MultiheadAttention` 实现的自注意力层。它可以处理图像的特征，使模型能够在图像的不同区域之间建立联系，这在解析复杂图像时非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 引入自注意机制后的图像编码器\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_channels, num_heads=8, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = nn.MultiheadAttention(num_channels, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 保存原始形状\n",
    "        orig_shape = x.shape\n",
    "        # 打印输入形状\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        # 转换为(sequence_length, batch_size, num_channels)格式\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        # 还原形状，确保与原始输入形状匹配\n",
    "        attention_output = attention_output.permute(1, 2, 0)# 打印最终输出形状\n",
    "        print(\"Final output shape:\", attention_output.shape)\n",
    "        return attention_output.view(orig_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47a042",
   "metadata": {},
   "source": [
    "2. 图像编码器 `ImageEncoder` 类：使用预训练的 ResNet-101 模型作为特征提取器，抽取图像的高层特征。**这些特征接着被自注意力层进一步处理，以增强图像区域间的相关性**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cda965",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True, num_heads=8, dropout=0.1):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # 使用ResNet101作为基础模型\n",
    "        model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        # 设置参数是否可训练\n",
    "        for param in self.grid_rep_extractor.parameters():\n",
    "            param.requires_grad = finetuned\n",
    "\n",
    "        # 自注意力层\n",
    "        self.self_attention = SelfAttention(model.fc.in_features, num_heads, dropout)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 通过ResNet网格表示提取器\n",
    "        features = self.grid_rep_extractor(images)\n",
    "        print(\"Extractor output shape:\", features.shape)\n",
    "        # 应用自注意力\n",
    "        features = self.self_attention(features)\n",
    "        # 打印自注意力输出形状\n",
    "        print(\"Self-attention output shape:\", features.shape)\n",
    "        return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113114c",
   "metadata": {},
   "source": [
    "3. 解码器的注意力机制 `AdditiveAttention` 类：实现了一种加法（或称为 Bahdanau）注意力机制，用于计算解码过程中的上下文向量。它通过比较解码器的隐藏状态（query）与图像编码（key-value）之间的关系来计算每个位置的注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 解码器的注意力机制\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim)\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim)\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1)\n",
    "        keys = self.attn_w_1_k(key_value)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2)\n",
    "        attn = self.softmax(attn)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69765d1d",
   "metadata": {},
   "source": [
    "4. 文本解码器 `AttentionDecoder` 类：定义了一个注意力机制的解码器，它结合了图像编码和前一个时间步的词嵌入来生成文本描述。解码器使用 GRU 单元进行序列生成，并且在每个时间步使用注意力权重来关注图像的不同区域，代码的关键在于使用`padded_predictions`来处理变长序列：\n",
    "\n",
    "**在生成文本描述时，不同的描述可能具有不同的长度。为了有效地处理这种变长序列，`padded_predictions` 被用来确保所有的输出序列具有相同的长度**。这样做对于批量处理和后续的损失计算至关重要。通过在`AttentionDecoder`中使用 `padded_predictions` 我们可以保证：\n",
    "\n",
    "- **确保统一长度**:\n",
    "  将所有的预测结果（即文本描述）填充到相同的最大长度。这是通过找出批次中最长的描述并将所有其他描述填充到这个长度来实现的。\n",
    "\n",
    "- **填充逻辑**:\n",
    "  对于每个描述，它只拷贝实际长度的预测结果到 `padded_predictions` 张量中。对于不足最大长度的部分，保持为零（默认填充值）。\n",
    "\n",
    "- **处理变长序列**:\n",
    "  这种填充方法允许模型处理变长的输出序列，同时保持批处理操作的一致性和效率。\n",
    "\n",
    "- **后续处理**:\n",
    "  `padded_predictions` 的使用使得后续的操作，如计算损失函数或评估模型性能，变得更加简单和直接，因为所有输出序列都有相同的维度。\n",
    "\n",
    "文本解码器 `AttentionDecoder`函数的关键代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 文本解码器\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "           初始化文本解码器。\n",
    "\n",
    "           参数:\n",
    "               image_code_dim: 图像编码的维度。\n",
    "               vocab_size: 词汇表的大小。\n",
    "               word_dim: 词嵌入的维度。\n",
    "               attention_dim: 注意力机制的隐藏层维度。\n",
    "               hidden_size: GRU隐藏层的大小。\n",
    "               num_layers: GRU层数。\n",
    "               dropout: Dropout层的概率。\n",
    "    \"\"\"\n",
    "    def forward(self, image_code, captions, cap_lens):\n",
    "    \"\"\"\n",
    "    完整的前馈过程。\n",
    "    参数：\n",
    "        hidden_state: (num_layers, batch_size, hidden_size)\n",
    "        image_code:  (batch_size, feature_channel, feature_size)\n",
    "        captions: (batch_size, )\n",
    "    \"\"\"\n",
    "    image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state \\\n",
    "        = self.init_hidden_state(image_code, captions, cap_lens)\n",
    "    batch_size = image_code.size(0)\n",
    "    # 输入序列长度减1，因为最后一个时刻不需要预测下一个词\n",
    "    lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "    # 初始化变量：模型的预测结果和注意力分数\n",
    "    max_cap_len = max(cap_lens)  # 计算最长caption的长度\n",
    "    predictions = torch.zeros(batch_size, max_cap_len, self.fc.out_features).to(captions.device)\n",
    "    alphas = torch.zeros(batch_size, max_cap_len, image_code.shape[1]).to(captions.device)\n",
    "    cap_embeds = self.embed(captions)\n",
    "    for step in range(lengths[0]):\n",
    "        real_batch_size = np.where(lengths > step)[0].shape[0]\n",
    "        preds, alpha, hidden_state = self.forward_step(\n",
    "            image_code[:real_batch_size],\n",
    "            cap_embeds[:real_batch_size, step, :],\n",
    "            hidden_state[:, :real_batch_size, :].contiguous())\n",
    "        # 记录结果\n",
    "        predictions[:real_batch_size, step, :] = preds\n",
    "        alphas[:real_batch_size, step, :] = alpha\n",
    "        max_cap_len = max(cap_lens)\n",
    "        # 初始化一个填充的predictions张量\n",
    "        padded_predictions = torch.zeros(batch_size, max_cap_len, self.fc.out_features).to(predictions.device)\n",
    "        for i in range(batch_size):\n",
    "            # 当前样本的实际长度\n",
    "            actual_length = cap_lens[i]\n",
    "            # 只拷贝实际长度的预测结果\n",
    "            padded_predictions[i, :actual_length, :] = predictions[i, :actual_length, :]\n",
    "    return padded_predictions, alphas, captions, lengths, sorted_cap_indices\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b785c",
   "metadata": {},
   "source": [
    "5. `AttentionModel`类：将图像编码器和文本解码器整合在一起，定义了完整的模型流程。在前向传递过程中，模型接受图像和文本描述，利用编码器和解码器生成描述的输出。通过引入`注意力机制`，使模型能够聚焦于输入图像的不同部分，根据图像的内容动态调整生成描述的重点；采用`束搜索算法`,考虑多个候选续词，并从中选取概率最高的几个以继续生成过程。这提高了生成文本的质量和相关性。其关键代码如下所示：\n",
    "    ```Python\n",
    "    class AttentionModel(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, attention_dim, hidden_size, num_layers):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e773bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, images, captions, cap_lens):\n",
    "    pass\n",
    "\n",
    "def generate_by_beamsearch(self, images, beam_k, max_len):\n",
    "    vocab_size = len(self.vocab)\n",
    "    image_codes = self.encoder(images)\n",
    "    texts = []\n",
    "    device = images.device\n",
    "    # 对每个图像样本执行束搜索\n",
    "    for image_code in image_codes:\n",
    "        # 将图像表示复制k份\n",
    "        image_code = image_code.unsqueeze(0).repeat(beam_k, 1, 1, 1)\n",
    "        # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "        cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)\n",
    "        cur_sent_embed = self.decoder.embed(cur_sents)[:, 0, :]\n",
    "        sent_lens = torch.LongTensor([1] * beam_k).to(device)\n",
    "        # 获得GRU的初始隐状态\n",
    "        image_code, cur_sent_embed, _, _, hidden_state = \\\n",
    "            self.decoder.init_hidden_state(image_code, cur_sent_embed, sent_lens)\n",
    "        # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "        end_sents = []\n",
    "        # 存储已生成完整的句子的概率\n",
    "        end_probs = []\n",
    "        # 存储未完整生成的句子的概率\n",
    "        probs = torch.zeros(beam_k, 1).to(device)\n",
    "        k = beam_k\n",
    "        while True:\n",
    "            preds, _, hidden_state = self.decoder.forward_step(image_code[:k], cur_sent_embed,hidden_state.contiguous())\n",
    "            preds = nn.functional.log_softmax(preds, dim=1)\n",
    "            # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "            probs = probs.repeat(1, preds.size(1)) + preds\n",
    "            if cur_sents.size(1) == 1:\n",
    "                # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                values, indices = probs[0].topk(k, 0, True, True)\n",
    "            else:\n",
    "                # probs: (k, vocab_size) 是二维张量\n",
    "                # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "            # 计算最大的k个值对应的句子索引和词索引\n",
    "            sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc')\n",
    "            word_indices = indices % vocab_size\n",
    "            # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "            cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "            # 查找此轮生成句子结束符<end>的句子\n",
    "            end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "            if len(end_indices) > 0:\n",
    "                end_probs.extend(values[end_indices])\n",
    "                end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                # 如果所有的句子都包含结束符，则停止生成\n",
    "                k -= len(end_indices)\n",
    "                if k == 0:\n",
    "                    break\n",
    "            # 查找还需要继续生成词的句子\n",
    "            cur_indices = [idx for idx, word in enumerate(word_indices)\n",
    "                           if word != self.vocab['<end>']]\n",
    "            if len(cur_indices) > 0:\n",
    "                cur_sent_indices = sent_indices[cur_indices]\n",
    "                cur_word_indices = word_indices[cur_indices]\n",
    "                # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                cur_sents = cur_sents[cur_indices]\n",
    "                probs = values[cur_indices].view(-1, 1)\n",
    "                hidden_state = hidden_state[:, cur_sent_indices, :]\n",
    "                cur_sent_embed = self.decoder.embed(\n",
    "                    cur_word_indices.view(-1, 1))[:, 0, :]\n",
    "            # 句子太长，停止生成\n",
    "            if cur_sents.size(1) >= max_len:\n",
    "                break\n",
    "        if len(end_sents) == 0:\n",
    "            # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "            gen_sent = cur_sents[0].tolist()\n",
    "        else:\n",
    "            # 否则选取包含结束符的句子中概率最大的句子\n",
    "            gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "        texts.append(gen_sent)\n",
    "    return texts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663dc88",
   "metadata": {},
   "source": [
    "6. 损失函数 `PackedCrossEntropyLoss` 类：为序列学习任务定义了交叉熵损失函数，忽略填充的部分。它使用了 `pack_padded_sequence` 来处理不同长度的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c027e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        计算交叉熵损失，排除填充的部分。\n",
    "        参数：\n",
    "            predictions：模型的预测结果，形状为 (batch_size, max_length, vocab_size)。\n",
    "            targets：实际的文本描述，形状为 (batch_size, max_length)。\n",
    "            lengths：每个描述的实际长度。\n",
    "        \"\"\"\n",
    "        packed_predictions = pack_padded_sequence(predictions, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "        packed_targets = pack_padded_sequence(targets, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 计算损失，忽略填充的部分\n",
    "        loss = self.loss_fn(packed_predictions, packed_targets)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338afa71",
   "metadata": {},
   "source": [
    "7. `evaluate_cider`类：评估一个图像描述生成模型的性能，使用 CIDEr-D（Consensus-based Image Description Evaluation - D）评分来衡量模型生成的文本描述与实际（参考）描述之间的一致性。首先使用`filter_useless_words` 函数去除无用词汇 （如 '<start>', '<end>', '<pad>'），之后再进行模型评估，接着生成描述并准备评估数据，最后组织评估数据并计算 CIDEr-D 得分。其关键代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与CIDEr-D计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "\n",
    "def evaluate_cider(data_loader, model, config):\n",
    "    model.eval()\n",
    "    # 存储候选文本和参考文本\n",
    "    cands = {}\n",
    "    refs = {}\n",
    "    filterd_words = {model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']}\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 加载词汇表并创建反向词汇表\n",
    "    with open('../data/output/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        # 通过束搜索生成描述\n",
    "        preds = model.generate_by_beamsearch(imgs, config.beam_k, config.max_len)\n",
    "        for j in range(imgs.size(0)):\n",
    "            img_id = str(i * config.batch_size + j)\n",
    "            cand_words = [idx_to_word.get(word, '<unk>') for word in preds[j]]\n",
    "            cand = ' '.join(filter_useless_words(cand_words, filterd_words))\n",
    "            cands[img_id] = [cand]  # 候选描述\n",
    "            # 将参考描述（caps[j]）的每个索引转换为单词\n",
    "            ref_words = [idx_to_word.get(word.item(), '<unk>') for word in caps[j]]\n",
    "            refs[img_id] = [' '.join(filter_useless_words(ref_words, filterd_words))]  # 参考描述\n",
    "    # 计算CIDEr-D得分\n",
    "    cider_evaluator = Cider()\n",
    "    score, _ = cider_evaluator.compute_score(refs, cands)\n",
    "    model.train()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea75891",
   "metadata": {},
   "source": [
    "### 4.2 网格/区域表示、Transformer编码器+Transformer解码器的模型结构：\n",
    "\n",
    "Transformer Model 整体架构图：\n",
    "\n",
    "![image](../doc/img/Transformer_framework.png)\n",
    "\n",
    "我们首先使用 argparse 库解析命令行参数，获取图像路径、模型版本和 Checkpoint 路径；其次根据命令行参数加载预训练模型，或者从 Checkpoint 加载模型（可选）；紧接着使用 PIL 库打开图像，并进行预处理；然后使用模型生成图像的描述；最后使用 METEOR 和 ROUGE-L 评估生成的描述与参考描述的相似度。\n",
    "\n",
    "我们定义了一个名为 `MyDataset` 的类，这个类继承自 PyTorch 的 `Dataset` 基类。在 `init` 方法中，这个类接受一个 json 文件的路径、一个图像目录的路径和一个可选的图像转换函数。json 文件中应该包含图像文件名和对应的标题。这个方法首先读取 json 文件并将其内容保存在 `self.data` 中，然后保存图像目录的路径和图像转换函数。最后，它从 `self.data` 中提取所有的文件名并保存在 `self.filenames` 中。`__len__` 方法返回数据集中的样本数量，这是通过返回 `self.data` 的长度来实现的。`__getitem__` 方法接受一个索引 `idx`，并返回对应的图像和标题。它首先从 `self.filenames` 中获取文件名，然后从 `self.data` 中获取对应的标题。接着，它打开对应的图像文件，并如果提供了图像转换函数，就对图像进行转换。最后，它返回图像和标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcb315",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        caption = self.data[filename]\n",
    "        image = Image.open(f\"{self.img_dir}/{filename}\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc233",
   "metadata": {},
   "source": [
    "#### 4.2.1 参数解析模块\n",
    "\n",
    "解析命令行参数，获取图像路径、模型版本和 Checkpoint 路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885e587",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Image Captioning')\n",
    "parser.add_argument('--path', type=str, help='path to image',\n",
    "required=True)\n",
    "parser.add_argument('--v', type=str, help='version')\n",
    "parser.add_argument('--checkpoint', type=str, help='checkpointpath', default=None)\n",
    "args = parser.parse_args()\n",
    "image_path = args.path\n",
    "version = args.v\n",
    "checkpoint_path = args.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c05620",
   "metadata": {},
   "source": [
    "#### 4.2.2 图像预处理模块\n",
    "\n",
    "使用 PIL 库打开图像，并进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37d098",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "start_token = tokenizer.convert_tokens_to_ids(tokenizer._cls_token)\n",
    "end_token = tokenizer.convert_tokens_to_ids(tokenizer._sep_token)\n",
    "image = Image.open(image_path)\n",
    "image = coco.val_transform(image)\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5773edb",
   "metadata": {},
   "source": [
    "#### 4.2.3 Caption 生成模块\n",
    "\n",
    "顾名思义，使用模型生成图像的描述。我们使用了骨干网络提取图像特征，Transformer 处理序列数据，而最终的标注生成则通过一个简单的多层感知机完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b85c99",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class Caption(nn.Module):\n",
    "    def __init__(self, backbone, transformer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.input_proj = nn.Conv2d(\n",
    "            backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.transformer = transformer\n",
    "        self.mlp = MLP(hidden_dim, 512, vocab_size, 3)\n",
    "\n",
    "    def forward(self, samples, target, target_mask):\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "\n",
    "        features, pos = self.backbone(samples)\n",
    "        src, mask = features[-1].decompose()\n",
    "\n",
    "        assert mask is not None\n",
    "\n",
    "        hs = self.transformer(self.input_proj(src), mask,\n",
    "                              pos[-1], target, target_mask)\n",
    "        out = self.mlp(hs.permute(1, 0, 2))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
    "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    backbone = build_backbone(config)\n",
    "    transformer = build_transformer(config)\n",
    "\n",
    "    model = Caption(backbone, transformer, config.hidden_dim, config.vocab_size)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, criterion\n",
    "\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "    return caption_template, mask_template\n",
    "\n",
    "caption, cap_mask = create_caption_and_mask(start_token,\n",
    "                                            config.max_position_embeddings)\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    for i in range(config.max_position_embeddings - 1):\n",
    "        predictions = model(image, caption, cap_mask)\n",
    "        predictions = predictions[:, i, :]\n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "        if predicted_id[0] == 102:\n",
    "\t\t    return caption\n",
    "        caption[:, i+1] = predicted_id[0]\n",
    "        cap_mask[:, i+1] = False\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c764d6",
   "metadata": {},
   "source": [
    "#### 4.2.4 Transformer模块\n",
    "\n",
    "1. **Transformer类:**\n",
    "    - 定义了一个包含编码器（`TransformerEncoder`）、解码器（`TransformerDecoder`）、嵌入层（`DecoderEmbeddings`）的整体Transformer模型。\n",
    "    - 接受一些配置参数，如模型的维度（`d_model`）、头数（`nhead`）、层数等。\n",
    "    - 在前向传播中，首先通过嵌入层处理目标序列（`tgt`），然后经过编码器和解码器，最终返回模型的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(\n",
    "            encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        self.embeddings = DecoderEmbeddings(config)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, mask, pos_embed, tgt, tgt_mask):\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
    "        mask = mask.flatten(1)\n",
    "\n",
    "        tgt = self.embeddings(tgt).permute(1, 0, 2)\n",
    "        query_embed = self.embeddings.position_embeddings.weight.unsqueeze(1)\n",
    "        query_embed = query_embed.repeat(1, bs, 1)\n",
    "\n",
    "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
    "        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask, tgt_key_padding_mask=tgt_mask,\n",
    "                          pos=pos_embed, query_pos=query_embed,\n",
    "                          tgt_mask=generate_square_subsequent_mask(len(tgt)).to(tgt.device))\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e669b8a",
   "metadata": {},
   "source": [
    "2. **TransformerEncoder类:**\n",
    "    - 该类继承自`nn.Module`，在其构造函数中接收一个编码器层（`encoder_layer`），以及编码器应该有的层数（`num_layers`）。还可选择性地包括一个层归一化组件（`norm`）。编码器的核心是其多层结构，由`_get_clones`函数实现，该函数创建了`encoder_layer`的多个副本，每个副本代表编码器的一个层。\n",
    "\n",
    "   - 在编码器的`forward`方法中，实现了对输入数据（`src`）的处理。处理过程遍历每个编码器层，将数据连续传递给每个层，同时可能应用层归一化。编码器层处理数据时，可以考虑额外的掩码（`mask`），用于指定序列中哪些部分应被模型忽略，以及位置编码（`pos`），它为模型提供关于序列中各元素位置的信息。最后，如果定义了层归一化（`norm`），则在最终输出之前应用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad73692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf86d3",
   "metadata": {},
   "source": [
    "3. **TransformerDecoder类:**\n",
    "    - 该类同样继承自`nn.Module`。在构造函数中，它接收一个解码器层（`decoder_layer`），解码器的层数（`num_layers`），可选的层归一化组件（`norm`），以及一个标志`return_intermediate`，用于决定是否返回每个解码层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 在`forward`方法中，该类处理目标序列（`tgt`）并与编码器输出的记忆（`memory`）进行交互。这一过程遍历解码器的每一层，每层都使用目标序列的掩码（`tgt_mask`）、记忆的掩码（`memory_mask`）、目标序列的键掩码（`tgt_key_padding_mask`）和记忆的键掩码（`memory_key_padding_mask`），以及位置编码（`pos`）和查询位置编码（`query_pos`）。这些掩码和位置编码提供了处理序列时所需的上下文信息和位置感知能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd8609",
   "metadata": {},
   "source": [
    "4. **TransformerEncoderLayer类:**\n",
    "    - `TransformerEncoderLayer`类是Transformer模型中编码器的单个层。这个类扩展了`nn.Module`，在其构造函数中初始化了多个组件，用于构成一个完整的编码器层。\n",
    "\n",
    "   - **主要组件**:\n",
    "     - `self_attn`: 多头注意力机制，用于在输入序列上自我关注。\n",
    "     - `linear1` 和 `linear2`: 两个线性层，构成前馈神经网络。\n",
    "     - `norm1` 和 `norm2`: 两个层归一化组件。\n",
    "     - `dropout`, `dropout1`, `dropout2`: 用于正则化的丢弃层。\n",
    "     - `activation`: 激活函数，由`_get_activation_fn`函数根据提供的激活函数名（如\"relu\"）确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31abf49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "- **前向传播**:\n",
    "  - `forward`方法根据`normalize_before`标志选择使用`forward_pre`或`forward_post`方法。\n",
    "  - `forward_pre`和`forward_post`实现了两种不同的处理顺序：要么先归一化再注意力和前馈（`forward_pre`），要么先注意力和前馈再归一化（`forward_post`）。\n",
    "  - 在这两种方法中，多头注意力和前馈网络的应用是核心处理步骤，同时还包括了位置编码的添加和必要的掩码处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6d1fc",
   "metadata": {},
   "source": [
    "- **位置编码**:\n",
    "     - `with_pos_embed`方法用于将位置编码加到输入张量上，这是Transformer模型处理序列数据的一个关键特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c400c",
   "metadata": {},
   "source": [
    "5. **TransformerDecoderLayer类:**\n",
    "    - `TransformerDecoderLayer`类是Transformer模型中解码器的单个层。这个类继承自`nn.Module`，在其构造函数中初始化了解码器层的关键组件，使得解码器能够处理目标序列并结合编码器的输出（记忆）生成最终输出。\n",
    "\n",
    "   - **主要组件**:\n",
    "     - `self_attn`: 一个多头自注意力机制，用于目标序列的内部处理。\n",
    "     - `multihead_attn`: 另一个多头注意力机制，用于目标序列与编码器输出（记忆）之间的交互。\n",
    "     - `linear1` 和 `linear2`: 两个线性层，构成前馈神经网络。\n",
    "     - `norm1`, `norm2`, `norm3`: 三个层归一化组件。\n",
    "     - `dropout`, `dropout1`, `dropout2`, `dropout3`: 用于正则化的丢弃层。\n",
    "     - `activation`: 激活函数，由`_get_activation_fn`函数确定。\n",
    "\n",
    "   - **前向传播**:\n",
    "     - `forward`方法根据`normalize_before`标志选择使用`forward_pre`或`forward_post`方法。\n",
    "     - `forward_pre`和`forward_post`实现了两种不同的处理顺序：要么先归一化再进行注意力和前馈处理（`forward_pre`），要么先进行注意力和前馈处理再归一化（`forward_post`）。\n",
    "     - 这两种方法都包括两个注意力机制的应用：一个用于目标序列内部的自注意力，另一个用于目标序列与编码器输出之间的交互。此外，还包括前馈网络的应用和必要的位置编码加和。\n",
    "\n",
    "   - **位置编码和掩码处理**:\n",
    "     - `with_pos_embed`方法用于将位置编码添加到输入张量上。\n",
    "     - 方法还处理了目标掩码（`tgt_mask`）、编码器输出的掩码（`memory_mask`）、目标序列的键掩码（`tgt_key_padding_mask`）和编码器输出的键掩码（`memory_key_padding_mask`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17e249",
   "metadata": {},
   "source": [
    "6. **DecoderEmbeddings类:**\n",
    "    - `DecoderEmbeddings`类是Transformer模型中用于解码器的嵌入层。这个类继承自`nn.Module`，在其构造函数中初始化了两种嵌入：词嵌入和位置嵌入，同时还包括了层归一化和丢弃层。\n",
    "\n",
    "   - **主要组件**:\n",
    "     - `word_embeddings`: 用于将输入词汇转换为固定维度的向量。这是通过查找每个词汇的索引在预定义的嵌入矩阵中对应的向量来实现的。\n",
    "     - `position_embeddings`: 提供了序列中每个位置的位置嵌入，这对于模型理解输入序列中单词的顺序是必要的。\n",
    "     - `LayerNorm`: 层归一化，用于标准化嵌入向量。\n",
    "     - `dropout`: 丢弃层，用于模型训练时的正则化。\n",
    "\n",
    "   - **初始化过程**:\n",
    "     - 嵌入层的大小和其他参数（如`vocab_size`、`hidden_dim`、`padding_idx`）是通过`config`配置对象传递的。\n",
    "\n",
    "   - **前向传播**:\n",
    "     - `forward`方法接收输入`x`（通常是词汇的索引）并计算其词嵌入和位置嵌入。\n",
    "     - 词嵌入和位置嵌入相加，得到每个词汇的最终嵌入表示。\n",
    "     - 然后对这些嵌入应用层归一化和丢弃操作，以增强模型的泛化能力和稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90091d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_dim, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_dim\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(\n",
    "            config.hidden_dim, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = x.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = x.device\n",
    "\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "\n",
    "        input_embeds = self.word_embeddings(x)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = input_embeds + position_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e6d58",
   "metadata": {},
   "source": [
    "### 4.3 ARCTIC模型\n",
    "为了与自己实现的模型进行效果上的对比，我复现了老师给的原始模型——`ARCTIC`的代码，**并成功在DeepFashion-MultiModal 数据集上进行训练，得到了相应的评估指标的得分**，模型性能具体的对比我将在下一部分进行重点分析。\n",
    "\n",
    "## 五、基于强化学习的损失函数的实现\n",
    "\n",
    "### 5.1 在Attention Model中实现的基于强化学习的损失函数：\n",
    "为了在Attention Model中实现的基于强化学习的损失函数，我使用了一种称为**自我临界序列训练**（`Self-Critical Sequence Training`, SCST）的**策略梯度方法**。这种方法在image-caption生成等序列生成任务中非常有效。通过自我临界的方式，模型被训练来优化生成序列的质量，使得其尽可能接近人类的参考描述。这种基于奖励的损失函数，与传统的像交叉熵这样的损失函数相比，更加关注整体序列的质量和流畅度。以下是实现细节的分析以及具体的实现的代码：\n",
    "\n",
    "- **ReinforceCriterion 类**:\n",
    "   - 这是一个继承自`nn.Module`的自定义损失函数类。\n",
    "   - 它接收一个CIDEr评估器（`cider_evaluator`），用于计算奖励。\n",
    "   - 类中还加载了一个词汇表，用于将预测的单词索引转换回单词字符串。\n",
    "\n",
    "- **前向传播 (`forward` 方法)**:\n",
    "   - 接收预测序列（`preds`）、图像输入（`imgs`）和实际描述（`caps`）。\n",
    "   - 使用softmax计算预测序列中每个单词的概率分布，并基于这个分布采样得到序列样本。\n",
    "   - 使用CIDEr评估器计算生成的描述（基于样本）和参考描述（实际描述）之间的CIDEr得分，这个得分作为奖励。\n",
    "\n",
    "- **计算损失**:\n",
    "   - 采用策略梯度方法计算损失函数。首先，计算采样动作的对数概率（`log_probs`）。\n",
    "   - 然后，将这些对数概率与相应的奖励（CIDEr得分）相乘，得到的损失是对所有样本的平均值。\n",
    "\n",
    "- **CIDEr得分作为奖励**:\n",
    "   - `_get_self_critical_reward` 方法使用CIDEr评估器来计算奖励。它比较生成的描述和参考描述，根据二者之间的CIDEr得分来评估质量。\n",
    "   - CIDEr得分越高，意味着生成的描述与参考描述越接近，因此奖励越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceCriterion(nn.Module):\n",
    "    def __init__(self, cider_evaluator):\n",
    "        super(ReinforceCriterion, self).__init__()\n",
    "        self.cider_evaluator = cider_evaluator\n",
    "        # 加载词汇表\n",
    "        with open('../data/output/vocab.json', 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def forward(self, preds, imgs, caps):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            preds: 预测序列，每个单词的分布 (batch_size, max_seq_length, vocab_size)\n",
    "            imgs: 图像输入，用于生成新的描述进行评分\n",
    "            caps: 实际描述，每张图片的一个描述 (batch_size, max_seq_length)\n",
    "        \"\"\"\n",
    "        batch_size = preds.size(0)\n",
    "        seq_length = preds.size(1)\n",
    "        vocab_size = preds.size(2)\n",
    "\n",
    "        # 生成序列的采样\n",
    "        probs = torch.softmax(preds, dim=2)\n",
    "        dists = Categorical(probs)\n",
    "        samples = dists.sample()\n",
    "\n",
    "        # 计算奖励（CIDEr-D得分）\n",
    "        gen_result, ref_result = self._prepare_for_cider(samples, imgs, caps)\n",
    "        reward = self._get_self_critical_reward(gen_result, ref_result)\n",
    "\n",
    "        # 计算损失\n",
    "        log_probs = dists.log_prob(samples)\n",
    "        loss = -log_probs * reward\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _prepare_for_cider(self, samples, imgs, caps):\n",
    "        gen_result = {}\n",
    "        ref_result = {}\n",
    "\n",
    "        # 假设 imgs 包含了图片的ID信息\n",
    "        image_ids = [img_id for img_id in imgs]\n",
    "\n",
    "        idx_to_word = {i: w for w, i in self.vocab.items()}\n",
    "        for idx, sample in enumerate(samples.tolist()):\n",
    "            image_id = str(image_ids[idx])\n",
    "            gen_sentence = ' '.join([idx_to_word.get(word, '<unk>') for word in sample if\n",
    "                                     word not in (self.vocab['<start>'], self.vocab['<end>'], self.vocab['<pad>'])])\n",
    "            gen_result[image_id] = [gen_sentence]\n",
    "\n",
    "            ref_sentence = ' '.join([idx_to_word.get(word, '<unk>') for word in caps[idx].tolist() if\n",
    "                                     word not in (self.vocab['<start>'], self.vocab['<end>'], self.vocab['<pad>'])])\n",
    "            ref_result[image_id] = [ref_sentence]\n",
    "\n",
    "        return gen_result, ref_result\n",
    "\n",
    "    def _get_self_critical_reward(self, gen_result, ref_result):\n",
    "        # 使用CIDEr-D评估器计算奖励\n",
    "        gen_score, _ = self.cider_evaluator.compute_score(ref_result, gen_result)\n",
    "        return gen_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade9ac9",
   "metadata": {},
   "source": [
    "### 5.2 在Transformer Model中实现的基于强化学习的损失函数\n",
    "为了实现基于强化学习的损失函数，我利用策略梯度方法训练一个image-caption生成模型，并通过对模型产生的每个caption单词的预测概率和实际匹配度进行奖励，从而引导模型学习生成更准确的caption。这种方法结合了深度学习和强化学习的技术，适用于解决序列生成类问题。\n",
    "- **奖励函数 (`reward_function`)**:\n",
    "   - 输入包括预测的字幕（`predictions`）和目标字幕（`targets`）。\n",
    "   - 函数返回一个布尔张量，其中每个元素表示相应的预测是否与目标匹配。\n",
    "   - 将布尔张量转换为浮点数，以便用于后续的数学计算。\n",
    "\n",
    "- **策略梯度更新函数 (`policy_gradient_update`)**:\n",
    "   - 首先，模型对给定的图像和字幕进行预测，输出字幕的概率分布（logits）。\n",
    "   - 使用`reward_function`计算奖励，基于模型预测和真实字幕是否匹配。\n",
    "   - 计算每个预测单词的动作概率（使用softmax函数）。\n",
    "   - 选择与真实字幕单词对应的动作概率。\n",
    "   - 计算损失函数，结合动作概率和奖励。这里使用的是策略梯度方法中的一个常见形式，即对数似然乘以奖励的平均值。\n",
    "   - 执行反向传播和优化器步骤，以更新模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce707fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(predictions, targets):\n",
    "    return (predictions == targets).float()\n",
    "\n",
    "def policy_gradient_update(model, images, captions, optimizer):\n",
    "    outputs = model(images, captions['input_ids'], captions['attention_mask'])\n",
    "    rewards = reward_function(outputs.logits.argmax(-1), captions['input_ids'])\n",
    "    action_probs = outputs.logits.softmax(-1)\n",
    "    picked_action_probs = action_probs.gather(-1, captions['input_ids'].unsqueeze(-1)).squeeze(-1)\n",
    "    loss = (-torch.log(picked_action_probs) * rewards).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaa4b3",
   "metadata": {},
   "source": [
    "## 六、实验结果与分析\n",
    "\n",
    "### 6.1 网格/区域表示、自注意力+注意力模型结构的实验结果及相应的分析\n",
    "- 在`DeepFashion-MultiModal`数据集上训练两种服饰图像描述模型（老师给的模型（ARCTIC），以及自己写的网格/区域表示、自注意力+注意力的模型结构（定义为`Attention Model`）），并对比这两种模型之间性能的差异：\n",
    "    - `ARCTIC`的训练过程及相应的`CIDEr-D`评分：\n",
    "    ![Alt text](../doc/img/OriginalModel-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "- `Attention Model`的训练过程及相应的`CIDEr-D`评分：\n",
    "![Alt text](../doc/img/AttentionModel-first_train-1.png)\n",
    "![Alt text](../doc/img/AttentionModel-first_train-2.png)\n",
    "![Alt text](../doc/img/AttentionModel-first_train-3.png)\n",
    "![Alt text](../doc/img/AttentionModel-first_train-4.png)\n",
    "\n",
    "- 结果分析：\n",
    "可以看到ARCTIC模型的CIDEr-D评分最高仅能达到**0.4118**，但是Attention Model在超参数相同的情况下CIDEr-D评分最高可以达到**1.3281**。两个模型在相同数据集上表现出显著不同的CIDEr-D评分，可能有以下几个原因：\n",
    "\n",
    "- **特征提取能力的差异**\n",
    "  - **ARCTIC** 使用了标准的ResNet-101作为图像编码器，只进行了**基本的特征提取**。\n",
    "  - **AttentionModel** 在ResNet-101基础上引入了自注意力机制。这种机制**允许模型更好地理解图像的全局上下文和复杂关系，从而产生更丰富和准确的图像特征表示**。\n",
    "\n",
    "- **自注意力机制的影响**\n",
    "自注意力机制使`Attention Model`能够**捕捉图像中不同区域之间的复杂关系**，这在处理具有丰富细节和多个对象的图像时特别有效。相比之下，`ARCTIC`可能无法同样有效地捕捉这些关系，从而影响其生成描述的质量。\n",
    "\n",
    "- **数据集特性**\n",
    "在**数据集中包含了许多复杂的场景或需要全局理解的图像，自注意力机制的优势更加明显**。`Attention Model`能够更好地解析这些复杂的图像，从而生成更准确的描述。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdafc1",
   "metadata": {},
   "source": [
    "- 利用训练的服饰图像描述模型，为真实背景的服饰图像数据集增加服饰描述，构建全新的服饰图像描述数据集，**在新数据集上重新训练服饰图像描述模型**：\n",
    "    - 使用普通的损失函数进行训练的结果：\n",
    "    ![Alt text](../doc/img/image20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 使用**基于强化学习的损失函数**进行训练的结果：\n",
    "![Alt text](../doc/img/image21.png)\n",
    "\n",
    "- 结果分析：\n",
    "根据图片中显示的训练日志和最终的CIDEr-D得分，我们可以推断出使用基于强化学习的损失函数（ReinforceCriterion）对模型性能的影响可能比使用常规损失函数更显著。我认为大致可以概括为以下几方面：\n",
    "\n",
    "  - 强化学习损失函数的影响\n",
    "\n",
    "    1. **直接优化评价指标**：\n",
    "       **强化学习损失函数直接使用了CIDEr-D评价指标作为奖励信号**，这意味着训练过程直接优化了生成的字幕的CIDEr-D得分。因此，**生成的caption质量与评价指标更加一致**，这可能导致在该指标上表现更出色。\n",
    "\n",
    "    2. **探索与利用（Exploration-Exploitation）**：\n",
    "       强化学习方法通常包含了对新策略的探索，这**允许模型有机会尝试并学习非标准或非最大似然预测的字幕生成路径**，这可能带来更高的评分。\n",
    "\n",
    "    3. **自我临界训练（Self-Critical Training）**：\n",
    "       ReinforceCriterion类似于自我临界训练方法，它**使用模型自身在贪心解码策略下的表现作为基线**。这种方法**减少了训练与推理阶段的不一致性**，有助于改善模型的评估指标。\n",
    "\n",
    "  - 常规损失函数的局限性\n",
    "\n",
    "    1. **固定目标**：\n",
    "       使用常规损失函数时，模型被训练去预测固定的下一个正确单词。**这种训练方式不会鼓励模型学习更丰富的语言模式或更好的语句结构**。\n",
    "\n",
    "    2. **评价指标不一致性**：\n",
    "       **常规损失函数通常不直接与最终的评价指标（如CIDEr-D）对齐**。虽然交叉熵损失可以有效地最小化预测误差，但不保证生成的描述在质量和多样性上是最优的。\n",
    "\n",
    "    3. **梯度信号**：\n",
    "       交叉熵损失提供了一个清晰的梯度信号用于每个时刻的单词预测，但它**可能不足以指导生成整个连贯的句子**，尤其是在与人类评估标准相关的方面。\n",
    "\n",
    "  - 分析总结\n",
    "\n",
    "    当使用基于强化学习的损失函数时，CIDEr-D得分大幅提高，这表明模型学会了生成更符合评估器标准的字幕。**强化学习方法使模型能够在一个更大的策略空间内进行探索，从而可能找到更优的字幕生成策略**，这些策略在常规训练方法中可能会被忽视。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459926c6",
   "metadata": {},
   "source": [
    "​    \n",
    "\n",
    "### 6.2 网格/区域表示、Transformer编码器+Transformer解码器模型结构的实验结果及相应的分析\n",
    "\n",
    "![image](../doc/img/Transformer_demo1.png)\n",
    "\n",
    "![image](../doc/img/Transformer_demo2.png)\n",
    "\n",
    "![image](../doc/img/Transformer_demo3.png)\n",
    "\n",
    "![image](../doc/img/Transformer_demo4.png)\n",
    "\n",
    "- 结果分析：可以看到，虽然看起来METEOR和ROUGE-L的分数还可以（经过调研，平均位于`[0.3, 0.6]`就算正常结果），不过通过模型生成的描述相对于参考描述有些短，这可能是因为以下原因：\n",
    "    1. 模型训练数据的偏差： 如果训练数据中的图像描述主要以简短的形式为主，模型可能更倾向于生成短的描述。这可能导致模型在生成时更倾向于较为简短的语言结构。解决这个问题的一种方法是使用更丰富和多样化的训练数据，确保模型能够学习到不同长度和类型的描述。\n",
    "    2. 训练目标的设定： 模型的训练目标可能与生成长描述的任务不一致。如果模型被设计为生成较短的描述，那么它可能会在生成时更倾向于保持短文本。在训练时，可以调整训练目标，使模型更加关注生成更长的、更详细的描述。\n",
    "    3. 模型架构的限制： Transformer模型本身可能存在对生成长序列的限制。虽然Transformer在处理长序列时有一些机制（比如自注意力机制），但是在实际应用中，长序列可能导致梯度消失或爆炸等问题。调整模型架构或使用更先进的模型可能有助于处理长序列生成任务。\n",
    "    4. 超参数的选择： 模型的超参数选择也可能影响到生成的描述长度。例如，训练时使用的最大序列长度、学习速率等超参数可能需要调整，以便更好地适应长描述的生成任务。\n",
    "\n",
    "### 6.3 BLIP 多模态输出展示及分析\n",
    "\n",
    "![image](../doc/img/BLIP_1.png)\n",
    "\n",
    "![image](../doc/img/BLIP_2.png)\n",
    "\n",
    "![image](../doc/img/BLIP_3.png)\n",
    "\n",
    "![image](../doc/img/BLIP_full.png)\n",
    "\n",
    "- 结果分析：对于原本背景内容比较丰富的图片的生成效果还可以，但是对于像是老师给的数据集，背景占比很小或者没什么实际内容的，效果就一般，原因简单而言如下：\n",
    "    1. 数据分布偏差： 如果训练数据主要包含背景内容丰富的图片，而很少包含背景较小或无实际内容的图片，模型可能更擅长处理前者而在后者上表现一般。这可能导致模型在生成背景较小的图片时效果较差。\n",
    "    2. 语言模型偏向： BLIP模型不仅仅是图像生成模型，还包含了对语言模型的预训练。如果语言模型的预训练数据中主要包含了与丰富背景相关的语言描述，模型可能更倾向于生成与这类背景相关的描述，而在其他情况下表现一般。\n",
    "    3. 图像生成的复杂性： BLIP模型可能在处理简单或缺乏实际内容的图像时面临生成的复杂性挑战。模型的能力受到其复杂性和参数设置的限制，因此可能难以有效地生成与这类图像相关的高质量描述。\n",
    "\n",
    "我个人认为解决这个问题的方法有：\n",
    "\n",
    "1. 更多样化的训练数据： 确保训练数据涵盖了各种背景情景，包括背景较小或无实际内容的图片，以提高模型在这些情况下的性能。\n",
    "2. 调整模型结构： 考虑调整BLIP模型的架构，使其更适应于处理不同类型的图像生成任务。\n",
    "3. 微调和迁移学习： 针对背景较小或无实际内容的图片，可以考虑使用微调或迁移学习的方法，以使模型更好地适应这些场景。\n",
    "4. 数据增强： 在训练过程中使用数据增强技术，可以通过对图像进行旋转、裁剪等操作，增加训练数据的多样性，从而提高模型对不同场景的适应能力。\n",
    "\n",
    "\n",
    "## 七、总结\n",
    "\n",
    "### 7.1 项目概览\n",
    "本项目围绕图像描述生成任务展开，主要涉及深度学习和神经网络技术的应用。项目目标是构建和训练模型，以生成准确且富有表现力的图像描述。实验中，我们不仅探索了传统的CNN-RNN结构，还实验了包括自注意力机制、Transformer模型和BLIP多模态模型在内的先进技术。\n",
    "\n",
    "### 7.2 技术实现\n",
    "- **模型结构**：本项目主要实验了两种模型结构：\n",
    "  1. **网格/区域表示、自注意力+注意力模型（Attention Model）**：此模型结构在传统ResNet-101基础上增加了自注意力机制，以更好地捕捉图像区域间的关系。\n",
    "  2. **网格/区域表示、Transformer编码器+Transformer解码器模型**：此模型采用了Transformer架构，能够有效处理图像和文本信息，以生成更准确的图像描述。\n",
    "\n",
    "- **损失函数**：项目中也探索了基于强化学习的损失函数（ReinforceCriterion），该方法通过优化与最终评价指标（如CIDEr-D）更一致的奖励信号来提高模型性能。\n",
    "\n",
    "### 7.3 实验结果与分析\n",
    "- **Attention Model vs ARCTIC**：实验结果显示，自注意力机制显著提高了Attention Model的性能，相比于ARCTIC模型，在CIDEr-D评分上取得了显著提升。\n",
    "- **基于强化学习的损失函数**：相较于传统损失函数，基于强化学习的损失函数能更有效地提升模型性能，尤其是在复杂场景和全局理解方面。\n",
    "- **Transformer模型**：Transformer模型展示了其在处理图像描述任务中的有效性，特别是在处理复杂图像和文本数据时。\n",
    "- **BLIP多模态模型**：BLIP模型通过整合视觉和语言信息，展示了其在多模态任务中的强大性能。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
