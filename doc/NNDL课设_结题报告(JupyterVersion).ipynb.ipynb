{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59efbf49",
   "metadata": {},
   "source": [
    "## 结题报告——Image Caption\n",
    "\n",
    "> 2023 秋季北京邮电大学深度学习与神经网络课程设计\n",
    "\n",
    "## 一、任务说明\n",
    "\n",
    "我们开发了一个基于编解码框架的图像描述生成系统。这个系统能够自动为输入的图片生成流畅且关联的自然语言描述。我们采用了 Show and Tell, Attention and Self-Attention, Transformer Encoder and Decoder 这三个模型结构来实现这个任务，接下来我们将在第四部分逐个介绍。\n",
    "\n",
    "至于什么是图像描述技术，其实就是以图像为输入，通过数学模型和计算使计算机输出对应图像的自然语言描述文字，使计算机拥有看图说话的能力，是图像处理领域中继图像识别、图像分割和目标跟踪之后的又一新型任务。在日常生活中，人们可以将图像中的场景、色彩、逻辑关系等低层视觉特征信息自动建立关系，从而感知图像的高层语义信息，但是计算机作为工具只能提取到数字图像的低层数据特征，而无法像人类大脑一样生成高层语义信息，这就是计算机视觉中的语义鸿沟问题。图像描述技术的本质就是将计算机提取的图像视觉特征转化为高层语义信息，即解决语义鸿沟问题，使计算机生成与人类大脑理解相近的对图像的文字描述，从而可以对图像进行分类、检索、分析等处理任务。\n",
    "\n",
    "我们通过完成这个课程设计作业，深入理解了编解码框架、自注意力机制、Transformer 模型等先进的深度学习技术，并能够将这些技术应用到实际问题中，不仅让我们更好的掌握了理论课上学到的知识，更锻炼了我们的动手实践能力。\n",
    "\n",
    "## 二、实验数据\n",
    "\n",
    "### 2.1 原始数据\n",
    "\n",
    "我们使用了 DeepFashion-MultiModal 数据集中 image 和 textual descriptions 的数据，其中 80% 的数据作为模型的训练集，20% 作为模型的测试集。数据集的 Github Repo 如下：\n",
    "\n",
    ">  https://github.com/yumingj/DeepFashion-MultiModal\n",
    "\n",
    "由于数据对应的 json 文件已经提前划分好，但是 images 文件夹仍然是混合在一起的，所以我们编写了一个 Python 脚本用于将 images 划分为 train_images 和 test_images，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c5182",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# 读取json文件并转换为字典\n",
    "with open('../../data/test_captions.json', 'r') as f:\n",
    "    test_captions = json.load(f)\n",
    "\n",
    "with open('../../data/train_captions.json', 'r') as f:\n",
    "    train_captions = json.load(f)\n",
    "\n",
    "# 指定源目录和目标目录\n",
    "source_directory = '../../data/images'\n",
    "train_directory = '../../data/train_images'\n",
    "test_directory = '../../data/test_images'\n",
    "\n",
    "# 确保目标目录存在\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "# 将训练集图片复制到目标目录\n",
    "for image in train_captions:\n",
    "    shutil.copy(os.path.join(source_directory, image), train_directory)\n",
    "\n",
    "# 将测试集图片复制到目标目录\n",
    "for image in test_captions:\n",
    "    shutil.copy(os.path.join(source_directory, image), test_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229241b",
   "metadata": {},
   "source": [
    "另外，我们还发现图像的关键点信息并没有在 json 文件中显示，而是在图像的文件名中，所以我们通过正则表达式，提取了每张图像的关键点信息，并更新了 json 文件，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecc5d3",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 定义一个函数来解析文件名\n",
    "def parse_filename(filename):\n",
    "    # 使用正则表达式匹配文件名\n",
    "    pattern = r'^(?P<gender>\\w+)-(?P<clothing>[\\w_]+)-id_(?P<id>\\d+)-(?P<group>\\d+)_(\\d+_(?P<body>\\w+))\\.jpg$'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 定义一个函数来处理目录中的所有文件\n",
    "def process_directory(directory):\n",
    "    # 创建一个字典来存储结果\n",
    "    results = {}\n",
    "    # 遍历目录中的所有文件\n",
    "    for filename in os.listdir(directory):\n",
    "        # 解析文件名\n",
    "        info = parse_filename(filename)\n",
    "        if info:\n",
    "            # 将解析的信息与文件名关联起来\n",
    "            results[filename] = info\n",
    "    return results\n",
    "\n",
    "# 使用函数处理目录\n",
    "directory = '../../data/images'\n",
    "results = process_directory(directory)\n",
    "\n",
    "# 将结果保存到json文件中\n",
    "with open('../../data/label.json', 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52126c",
   "metadata": {},
   "source": [
    "### 2.2 背景描述增量数据\n",
    "\n",
    "我们使用老师在群里提供的数据（一开始我们自己找了一个，但是发现不合适）。\n",
    "\n",
    "> https://pan.baidu.com/s/1qN3EEUNXh4nUcNZMCoT9Fg?pwd=rnfw (rnfw)\n",
    "\n",
    "同样地，我们将其重命名并划分为 train_images 和 test_images，比例为 9:1。\n",
    "\n",
    "![image](../doc/img/Ex_data.png)\n",
    "\n",
    "图片重命名的代码片段如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55195f97",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_images(folder_path):\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"文件夹 '{folder_path}' 不存在。\")\n",
    "        return\n",
    "\n",
    "    # 获取文件夹下所有文件\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # 迭代处理每个文件\n",
    "    for index, file_name in enumerate(files):\n",
    "        old_path = os.path.join(folder_path, file_name)\n",
    "        new_name = f\"train_{index + 1}.jpg\"\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"重命名文件: {file_name} -> {new_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c6daa",
   "metadata": {},
   "source": [
    "## 三、实验环境\n",
    "\n",
    "- Ubuntu 20.04 / Ubuntu 22.04 / Windows 11\n",
    "- NVIDIA GPU and NVIDIA CUDA Driver\n",
    "- CUDA 12.2 / CUDA 11.8\n",
    "- Python 3.10\n",
    "\n",
    "具体的第三方库依赖请见 `requirements.txt`\n",
    "\n",
    "## 四、模型选择\n",
    "\n",
    "首先，作为共性特点，我们先来介绍一下基于编解码器的方法：\n",
    "\n",
    "随着深度学习技术的不断发展，神经网络在计算机视觉和自然语言处理领域得到了广泛应用。受机器翻译领域中编解码器模型的启发，图像描述可以通过端到端的学习方法直接实现图像和描述句子之间的映射，将图像描述过程转化成为图像到描述的\"翻译\"过程。基于深度学习的图像描述生成方法大多采用以 CNN-RNN 为基本模型的编解码器框架，CNN 决定了整个模型的图像识别能力，其最后的隐藏层的输出被用作解码器的输入，RNN 是用来读取编码后的图像并生成文本描述的网络模型，下图是一个简单递归神经网络 RNN 和多模态递归神经网络 m-RNN 架构的示意图：\n",
    "\n",
    "![image](../doc/img/01.png)\n",
    "\n",
    "之所以是 CNN 决定了整个模型的图像识别能力，RNN 被用来读取编码后的图像并生成文本描述的网络模型，是因为计算机视觉问题有很强的局部特征，即一个像素和它周围的旁边几个像素有很强的关联性，但是离他非常远的像素之间的关联性就比较弱，所以只需要像 CNN 一样在局部做连接，而不需要像全连接网络一样每一层之间都是全连接的，从而大大降低了权重的数量。而 RNN 通过使用带自反馈的神经元，也就是隐藏状态，能够处理任意长度的序列数据，可以有效保存序列数据的历史信息。\n",
    "\n",
    "![image](../doc/img/CNN.png)\n",
    "\n",
    "![image](../doc/img/RNN.png)\n",
    "\n",
    "### 4.1 Show and Tell（就是老师给的那个）\n",
    "\n",
    "> 待补充\n",
    "\n",
    "### 4.2 Attention and Self Attention\n",
    "\n",
    "近年来，注意力机制被广泛应用于计算机视觉领域，其本质是为了解决编解码器在处理固定长度向量时的局限性。注意力机制并不是将输入序列编码成一个固定向量，而是通过增加一个上下文向量来对每个时间步的输入进行解码，以增强图像区域和单词的相关性，从而获取更多的图像语义细节，下图是一个学习单词 / 图像对齐过程的示意图：\n",
    "\n",
    "![image](../doc/img/02.png)\n",
    "\n",
    "我们利用网格 / 区域表示 + 自注意力 + 注意力的模型结构来完成副使图像描述任务。\n",
    "\n",
    "#### 4.2.1 参数配置方面\n",
    "\n",
    "`configurations.py` 文件中定义的 `Config` 类作为项目的配置中心，其作用是集中管理项目中使用的所有配置参数。这些参数通常包括文件路径、模型参数、数据处理选项、训练设置和图像处理参数等。通过这种方式，可以在不修改代码的情况下调整项目的行为。以下是 `Config` 类中定义的配置参数及其作用：\n",
    "\n",
    "1. **数据路径**：\n",
    "   - `data_path`：主数据目录路径。\n",
    "   - `images_path`：存储图像的路径。\n",
    "   - `train_captions_path`：训练集的文本描述文件路径。\n",
    "   - `test_captions_path`：测试集的文本描述文件路径。\n",
    "   - `output_folder`：用于存储词汇表和处理后数据的输出文件夹路径。\n",
    "\n",
    "2. **模型参数**：\n",
    "   - `embed_size`：嵌入向量的维度。\n",
    "   - `vocab_size`：词汇表的大小。\n",
    "   - `num_layers`：定义循环神经网络中的层数。\n",
    "   - `num_heads`：自注意力机制中头的数量。\n",
    "   - `dropout`：在模型中使用的 Dropout 比率。\n",
    "   - `hidden_size`：隐藏层的维度。\n",
    "   - `image_code_dim`：图像编码的维度。\n",
    "   - `word_dim`：词嵌入的维度。\n",
    "   - `attention_dim`：注意力机制的隐藏层维度。\n",
    "\n",
    "3. **数据处理参数**：\n",
    "   - `min_word_count`：词汇表中词的最小出现次数，用于筛选词汇。\n",
    "   - `max_len`：假设的描述的最大长度。\n",
    "\n",
    "4. **训练参数**：\n",
    "   - `batch_size`：每个批次的大小。\n",
    "   - `learning_rate`：学习率。\n",
    "   - `num_epochs`：训练的总轮次数。\n",
    "   - `workers`：加载数据时使用的工作线程数。\n",
    "   - `encoder_learning_rate`：编码器的学习率。\n",
    "   - `decoder_learning_rate`：解码器的学习率。\n",
    "   - `lr_update`：学习率更新频率。\n",
    "\n",
    "5. **图像预处理参数**：\n",
    "   - `image_size`：图像缩放后的大小。\n",
    "   - `crop_size`：从缩放后的图像中裁剪出的大小。\n",
    "\n",
    "6. **其他配置**：\n",
    "   - `device`：设置运行计算的设备，如果 CUDA 可用则使用 GPU，否则使用 CPU。\n",
    "\n",
    "#### 4.2.2 数据预处理方面\n",
    "\n",
    "为图像描述任务准备和预处理数据，确保数据能够被模型以适当的格式接受和处理，数据预处理是建立有效的训练和测试环境的基础。我们实现了 `datasets.py` 文件来进行数据预处理，以下是主要功能：\n",
    "\n",
    "1. `create_dataset` 函数：用于处理原始文本描述，创建一个词汇表，并将文本转换为对应的词索引向量。它首先读取训练和测试数据集中的文本描述，然后统计词频以创建词汇表，并移除低频词。之后，它定义了一个内部函数 `encode_captions`，这个函数负责将每条文本描述转换为一个固定长度的词索引序列，包括特殊标记 <start>, <end>, <pad>, 和 <unk>。转换完成后，函数将这些数据保存为 json 文件，以便后续处理。部分代码展示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "def create_dataset(max_len=64):\n",
    "    \"\"\"\n",
    "    整理数据集，构建词汇表，并将文本描述转换为词索引向量。\n",
    "    使用configuration.py文件中定义的配置信息。\n",
    "    \"\"\"\n",
    "    # 使用config中定义的路径\n",
    "    image_folder = config.images_path\n",
    "    train_captions_path = config.train_captions_path\n",
    "    test_captions_path = config.test_captions_path\n",
    "    output_folder = config.output_folder\n",
    "\n",
    "    # 读取训练图像描述\n",
    "    with open(train_captions_path, 'r') as f:\n",
    "        train_captions_data = json.load(f)\n",
    "\n",
    "    # 读取测试图像描述\n",
    "    with open(test_captions_path, 'r') as f:\n",
    "        test_captions_data = json.load(f)\n",
    "\n",
    "    # 统计训练集的文本描述的词频\n",
    "    vocab = Counter()\n",
    "    for caption in train_captions_data.values():\n",
    "        vocab.update(caption.lower().split())\n",
    "\n",
    "    # 移除其中的低频词\n",
    "    vocab = {word for word, count in vocab.items() if count >= config.min_word_count}\n",
    "\n",
    "    # 构建词典\n",
    "    word_to_idx = {word: idx + 4 for idx, word in enumerate(vocab)}\n",
    "    word_to_idx['<pad>'] = 0\n",
    "    word_to_idx['<start>'] = 1\n",
    "    word_to_idx['<end>'] = 2\n",
    "    word_to_idx['<unk>'] = 3\n",
    "\n",
    "    # 一个函数来转换描述为词索引向量，并进行填充\n",
    "    def encode_captions(captions_data, word_to_idx, max_len):\n",
    "        encoded_captions = {}\n",
    "        caplens = {}\n",
    "        for img_id, caption in captions_data.items():\n",
    "            words = caption.lower().split()\n",
    "            encoded_caption = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "            caplen = len(encoded_caption) + 2  # 加2是因为还要加上<start>和<end>\n",
    "            encoded_caption = [word_to_idx['<start>']] + encoded_caption + [word_to_idx['<end>']]\n",
    "            encoded_caption += [word_to_idx['<pad>']] * (max_len - len(encoded_caption))\n",
    "            encoded_captions[img_id] = encoded_caption[:max_len]\n",
    "            caplens[img_id] = caplen if caplen <= max_len else max_len\n",
    "        return encoded_captions, caplens\n",
    "\n",
    "    # 对训练集描述进行编码\n",
    "    encoded_captions_train, caplens_train = encode_captions(train_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 对测试集描述进行编码\n",
    "    encoded_captions_test, caplens_test = encode_captions(test_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 存储词典和编码后的描述\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as f:\n",
    "        json.dump(word_to_idx, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_train.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_test.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_test, f)\n",
    "\n",
    "    # 存储图像路径\n",
    "    image_paths_train = {img_id: os.path.join(image_folder, img_id) for img_id in train_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_train.json'), 'w') as f:\n",
    "        json.dump(image_paths_train, f)\n",
    "\n",
    "    image_paths_test = {img_id: os.path.join(image_folder, img_id) for img_id in test_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_test.json'), 'w') as f:\n",
    "        json.dump(image_paths_test, f)\n",
    "\n",
    "    # 存储caplens\n",
    "    with open(os.path.join(output_folder, 'caplens_train.json'), 'w') as f:\n",
    "        json.dump(caplens_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'caplens_test.json'), 'w') as f:\n",
    "        json.dump(caplens_test, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58155a4",
   "metadata": {},
   "source": [
    "2. `ImageTextDataset` 类：继承自 `torch.utils.data.Dataset`，这个类是一个 PyTorch 的自定义数据集，用于加载图像和对应的已编码文本描述。它重写了 `__getitem__` 方法，用于获取索引对应的数据点（图像和文本描述），并将图像通过转换处理成统一的格式；重写了 `__len__` 方法，返回数据集的大小，部分代码展示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch数据集类，用于加载和处理图像-文本数据。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths_file, captions_file, caplens_file, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集类。\n",
    "        参数:\n",
    "            image_paths_file: 包含图像路径的json文件路径。\n",
    "            captions_file: 包含编码后文本描述的json文件路径。\n",
    "            transform: 应用于图像的预处理转换。\n",
    "        \"\"\"\n",
    "        # 载入图像路径和文本描述以及caplens\n",
    "        with open(image_paths_file, 'r') as f:\n",
    "            self.image_paths = json.load(f)\n",
    "\n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "        with open(caplens_file, 'r') as f:\n",
    "            self.caplens = json.load(f)\n",
    "\n",
    "        # 设置图像预处理方法\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取单个数据点。\n",
    "        参数:\n",
    "            index: 数据点的索引。\n",
    "        返回:\n",
    "            一个包含图像和对应文本描述的元组。\n",
    "        \"\"\"\n",
    "        # 获取图像路径和文本描述以及caplen\n",
    "        image_id = list(self.image_paths.keys())[index]\n",
    "        image_path = self.image_paths[image_id]\n",
    "        caption = self.captions[image_id]\n",
    "        caplen = self.caplens[image_id]\n",
    "\n",
    "        # 加载图像并应用预处理\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 将文本描述转换为张量\n",
    "        caption_tensor = torch.tensor(caption, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tensor, caplen\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        数据集中的数据点总数。\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19153735",
   "metadata": {},
   "source": [
    "3. `create_dataloaders` 函数：使用 `ImageTextDataset` 类来创建PyTorch的 `DataLoader`，它提供了一个可迭代的数据加载器，用于在训练和测试时批量加载数据，并可选地对数据进行打乱和多进程加载，部分代码展示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 创建训练集和测试集的 DataLoader\n",
    "def create_dataloaders(config):\n",
    "    \"\"\"\n",
    "    创建训练集和测试集的 DataLoader。\n",
    "\n",
    "    参数:\n",
    "        batch_size: 每个批次的大小。\n",
    "        num_workers: 加载数据时使用的进程数。\n",
    "        shuffle_train: 是否打乱训练数据。\n",
    "\n",
    "    返回:\n",
    "        train_loader: 训练数据的 DataLoader。\n",
    "        test_loader: 测试数据的 DataLoader。\n",
    "    \"\"\"\n",
    "    # 图像预处理转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 加载数据时使用的进程数\n",
    "    num_workers = 0\n",
    "\n",
    "    # 创建数据集对象\n",
    "    train_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_train.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_train.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_train.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_test.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_test.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_test.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # 创建 DataLoader 对象\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,  # 通常测试集不需要打乱\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738973f2",
   "metadata": {},
   "source": [
    "4. 我还定义了一个 `datasets_pretrain_demo.py` 文件来验证数据预处理过程是否正确。它通过以下步骤实现这一目标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cf950",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 读取词汇表和编码后的描述：加载之前生成的词汇表 `vocab.json`，编码后的训练集描述 `encoded_captions_train.json`，以及训练图像的路径 `image_paths_train.json`。\n",
    "- 索引到单词的转换： 创建从词索引到单词的反向映射，用于将编码后的描述转换回文本形式。\n",
    "- 选择并展示图像： 从图像路径列表中选择第一个图像 ID，并加载对应的图像。\n",
    "- 展示图像：使用 matplotlib 展示图像，并关闭坐标轴。\n",
    "- 打印文本描述：将编码后的描述（词索引列表）转换回单词形式，并打印出来，以验证编码和图像加载的正确性。\n",
    "\n",
    "```Python\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_path = '../data/output/vocab.json'\n",
    "encoded_captions_path = '../data/output/encoded_captions_train.json'\n",
    "image_paths_path = '../data/output/image_paths_train.json'\n",
    "\n",
    "# 读取词典、编码后的描述和图像路径\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open(encoded_captions_path, 'r') as f:\n",
    "    encoded_captions = json.load(f)\n",
    "\n",
    "with open(image_paths_path, 'r') as f:\n",
    "    image_paths = json.load(f)\n",
    "\n",
    "# 将索引转换回单词\n",
    "vocab_idx2word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# 选择要展示的图片ID，这里以第一个ID为例\n",
    "first_img_id = list(image_paths.keys())[0]\n",
    "content_img = Image.open(image_paths[first_img_id])\n",
    "\n",
    "# 展示图片和对应的描述\n",
    "plt.imshow(content_img)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "# 打印对应的文本描述，确保字典中的键是整数，直接使用整数索引\n",
    "caption = ' '.join([vocab_idx2word[word_idx] for word_idx in encoded_captions[first_img_id]])\n",
    "print(caption)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a433cc",
   "metadata": {},
   "source": [
    "#### 4.2.3 模型定义\n",
    "\n",
    "我们的 `models.py` 文件定义了用于服饰图像描述任务的神经网络模型，包括图像编码器、注意力机制、文本解码器、整体模型框架和损失函数。以下是代码中各个部分的详细作用：\n",
    "\n",
    "1. 自注意力机制 `SelfAttention` 类：定义了一个利用 `nn.MultiheadAttention` 实现的自注意力层。它可以处理图像的特征，使模型能够在图像的不同区域之间建立联系，这在解析复杂图像时非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b428383",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 引入自注意机制后的图像编码器\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_channels, num_heads=8, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = nn.MultiheadAttention(num_channels, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 保存原始形状\n",
    "        orig_shape = x.shape\n",
    "        # 打印输入形状\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        # 转换为(sequence_length, batch_size, num_channels)格式\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        # 还原形状，确保与原始输入形状匹配\n",
    "        attention_output = attention_output.permute(1, 2, 0)# 打印最终输出形状\n",
    "        print(\"Final output shape:\", attention_output.shape)\n",
    "        return attention_output.view(orig_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b68b6d",
   "metadata": {},
   "source": [
    "2. 图像编码器 `ImageEncoder` 类：使用预训练的 ResNet-101 模型作为特征提取器，抽取图像的高层特征。这些特征接着被自注意力层进一步处理，以增强图像区域间的相关性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True, num_heads=8, dropout=0.1):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # 使用ResNet101作为基础模型\n",
    "        model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        # 设置参数是否可训练\n",
    "        for param in self.grid_rep_extractor.parameters():\n",
    "            param.requires_grad = finetuned\n",
    "\n",
    "        # 自注意力层\n",
    "        self.self_attention = SelfAttention(model.fc.in_features, num_heads, dropout)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 通过ResNet网格表示提取器\n",
    "        features = self.grid_rep_extractor(images)\n",
    "        print(\"Extractor output shape:\", features.shape)\n",
    "        # 应用自注意力\n",
    "        features = self.self_attention(features)\n",
    "        # 打印自注意力输出形状\n",
    "        print(\"Self-attention output shape:\", features.shape)\n",
    "        return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e31de",
   "metadata": {},
   "source": [
    "3. 解码器的注意力机制 `AdditiveAttention` 类)：实现了一种加法（或称为 Bahdanau）注意力机制，用于计算解码过程中的上下文向量。它通过比较解码器的隐藏状态（query）与图像编码（key-value）之间的关系来计算每个位置的注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1824e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 解码器的注意力机制\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim)\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim)\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1)\n",
    "        keys = self.attn_w_1_k(key_value)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2)\n",
    "        attn = self.softmax(attn)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c513cf",
   "metadata": {},
   "source": [
    "4. 文本解码器 `AttentionDecoder` 类：定义了一个注意力机制的解码器，它结合了图像编码和前一个时间步的词嵌入来生成文本描述。解码器使用 GRU 单元进行序列生成，并且在每个时间步使用注意力权重来关注图像的不同区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3990db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 文本解码器\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "           初始化文本解码器。\n",
    "\n",
    "           参数:\n",
    "               image_code_dim: 图像编码的维度。\n",
    "               vocab_size: 词汇表的大小。\n",
    "               word_dim: 词嵌入的维度。\n",
    "               attention_dim: 注意力机制的隐藏层维度。\n",
    "               hidden_size: GRU隐藏层的大小。\n",
    "               num_layers: GRU层数。\n",
    "               dropout: Dropout层的概率。\n",
    "    \"\"\"\n",
    "    def __init__(self, image_code_dim, vocab_size, word_dim, attention_dim, hidden_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, word_dim)\n",
    "        self.attention = AdditiveAttention(hidden_size, image_code_dim, attention_dim)\n",
    "        self.init_state = nn.Linear(image_code_dim, num_layers * hidden_size)\n",
    "        self.rnn = nn.GRU(word_dim + image_code_dim, hidden_size, num_layers)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        # RNN默认已初始化\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        初始化隐藏状态。\n",
    "\n",
    "        参数：\n",
    "            image_code：图像编码器输出的图像表示\n",
    "                        (batch_size, image_code_dim, grid_height, grid_width)\n",
    "            captions: 文本描述。\n",
    "            cap_lens: 文本描述的长度。\n",
    "        \"\"\"\n",
    "        # 将图像网格表示转换为序列表示形式\n",
    "        batch_size, image_code_dim = image_code.size(0), image_code.size(1)\n",
    "        # -> (batch_size, grid_height, grid_width, image_code_dim)\n",
    "        image_code = image_code.permute(0, 2, 3, 1)\n",
    "        # -> (batch_size, grid_height * grid_width, image_code_dim)\n",
    "        image_code = image_code.view(batch_size, -1, image_code_dim)\n",
    "        # （1）按照caption的长短排序\n",
    "        sorted_cap_lens, sorted_cap_indices = torch.sort(cap_lens, 0, True)\n",
    "        captions = captions[sorted_cap_indices]\n",
    "        image_code = image_code[sorted_cap_indices]\n",
    "        # （2）初始化隐状态\n",
    "        hidden_state = self.init_state(image_code.mean(axis=1))\n",
    "        hidden_state = hidden_state.view(\n",
    "            batch_size,\n",
    "            self.rnn.num_layers,\n",
    "            self.rnn.hidden_size).permute(1, 0, 2)\n",
    "        return image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state\n",
    "\n",
    "    def forward_step(self, image_code, curr_cap_embed, hidden_state):\n",
    "        \"\"\"\n",
    "                解码器的前馈步骤。\n",
    "\n",
    "                参数:\n",
    "                    image_code: 图像编码。\n",
    "                    curr_cap_embed: 当前时间步的词嵌入向量。\n",
    "                    hidden_state: 当前的隐藏状态。\n",
    "                \"\"\"\n",
    "        # （3.2）利用注意力机制获得上下文向量\n",
    "        # query：hidden_state[-1]，即最后一个隐藏层输出 (batch_size, hidden_size)\n",
    "        # context: (batch_size, hidden_size)\n",
    "        context, alpha = self.attention(hidden_state[-1], image_code)\n",
    "        # （3.3）以上下文向量和当前时刻词表示为输入，获得GRU输出\n",
    "        x = torch.cat((context, curr_cap_embed), dim=-1).unsqueeze(0)\n",
    "        # x: (1, real_batch_size, hidden_size+word_dim)\n",
    "        # out: (1, real_batch_size, hidden_size)\n",
    "        out, hidden_state = self.rnn(x, hidden_state)\n",
    "        # （3.4）获取该时刻的预测结果\n",
    "        # (real_batch_size, vocab_size)\n",
    "        preds = self.fc(self.dropout(out.squeeze(0)))\n",
    "        return preds, alpha, hidden_state\n",
    "\n",
    "    def forward(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        完整的前馈过程。\n",
    "\n",
    "        参数：\n",
    "            hidden_state: (num_layers, batch_size, hidden_size)\n",
    "            image_code:  (batch_size, feature_channel, feature_size)\n",
    "            captions: (batch_size, )\n",
    "        \"\"\"\n",
    "        # （1）将图文数据按照文本的实际长度从长到短排序\n",
    "        # （2）获得GRU的初始隐状态\n",
    "        image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state \\\n",
    "            = self.init_hidden_state(image_code, captions, cap_lens)\n",
    "        batch_size = image_code.size(0)\n",
    "        # 输入序列长度减1，因为最后一个时刻不需要预测下一个词\n",
    "        lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "        # 初始化变量：模型的预测结果和注意力分数\n",
    "        predictions = torch.zeros(batch_size, lengths[0], self.fc.out_features).to(captions.device)\n",
    "        alphas = torch.zeros(batch_size, lengths[0], image_code.shape[1]).to(captions.device)\n",
    "        # 获取文本嵌入表示 cap_embeds: (batch_size, num_steps, word_dim)\n",
    "        cap_embeds = self.embed(captions)\n",
    "        # Teacher-Forcing模式\n",
    "        for step in range(lengths[0]):\n",
    "            # （3）解码\n",
    "            # （3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths > step)[0].shape[0]\n",
    "            preds, alpha, hidden_state = self.forward_step(\n",
    "                image_code[:real_batch_size],\n",
    "                cap_embeds[:real_batch_size, step, :],\n",
    "                hidden_state[:, :real_batch_size, :].contiguous())\n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "            alphas[:real_batch_size, step, :] = alpha\n",
    "        return predictions, alphas, captions, lengths, sorted_cap_indices\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3df90",
   "metadata": {},
   "source": [
    "5. 整体模型 `ARCTIC` 类：将图像编码器和文本解码器整合在一起，定义了完整的模型流程。在前向传递过程中，模型接受图像和文本描述，利用编码器和解码器生成描述的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# ARCTIC 模型\n",
    "class ARCTIC(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, attention_dim, hidden_size, num_layers):\n",
    "        super(ARCTIC, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.decoder = AttentionDecoder(image_code_dim, len(vocab), word_dim, attention_dim, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions, cap_lens):\n",
    "        # 打印图像输入形状\n",
    "        print(\"Image input shape:\", images.shape)\n",
    "        image_code = self.encoder(images)\n",
    "        # 打印编码器输出形状\n",
    "        print(\"Encoder output shape:\", image_code.shape)\n",
    "        output = self.decoder(image_code, captions, cap_lens)\n",
    "        # 打印解码器输出形状\n",
    "        print(\"Decoder output shape:\", output[0].shape)  # Assuming output[0] is the main output\n",
    "        return output\n",
    "\n",
    "    def generate_by_beamsearch(self, images, beam_k, max_len):\n",
    "        vocab_size = len(self.vocab)\n",
    "        image_codes = self.encoder(images)\n",
    "        texts = []\n",
    "        device = images.device\n",
    "        # 对每个图像样本执行束搜索\n",
    "        for image_code in image_codes:\n",
    "            # 将图像表示复制k份\n",
    "            image_code = image_code.unsqueeze(0).repeat(beam_k, 1, 1, 1)\n",
    "            # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "            cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)\n",
    "            cur_sent_embed = self.decoder.embed(cur_sents)[:, 0, :]\n",
    "            sent_lens = torch.LongTensor([1] * beam_k).to(device)\n",
    "            # 获得GRU的初始隐状态\n",
    "            image_code, cur_sent_embed, _, _, hidden_state = \\\n",
    "                self.decoder.init_hidden_state(image_code, cur_sent_embed, sent_lens)\n",
    "            # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "            end_sents = []\n",
    "            # 存储已生成完整的句子的概率\n",
    "            end_probs = []\n",
    "            # 存储未完整生成的句子的概率\n",
    "            probs = torch.zeros(beam_k, 1).to(device)\n",
    "            k = beam_k\n",
    "            while True:\n",
    "                preds, _, hidden_state = self.decoder.forward_step(image_code[:k], cur_sent_embed,\n",
    "                                                                   hidden_state.contiguous())\n",
    "                # -> (k, vocab_size)\n",
    "                preds = nn.functional.log_softmax(preds, dim=1)\n",
    "                # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "                # -> (k, vocab_size)\n",
    "                probs = probs.repeat(1, preds.size(1)) + preds\n",
    "                if cur_sents.size(1) == 1:\n",
    "                    # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                    values, indices = probs[0].topk(k, 0, True, True)\n",
    "                else:\n",
    "                    # probs: (k, vocab_size) 是二维张量\n",
    "                    # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                    # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                    values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "                # 计算最大的k个值对应的句子索引和词索引\n",
    "                sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc')\n",
    "                word_indices = indices % vocab_size\n",
    "                # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "                cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "                # 查找此轮生成句子结束符<end>的句子\n",
    "                end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "                if len(end_indices) > 0:\n",
    "                    end_probs.extend(values[end_indices])\n",
    "                    end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                    # 如果所有的句子都包含结束符，则停止生成\n",
    "                    k -= len(end_indices)\n",
    "                    if k == 0:\n",
    "                        break\n",
    "                # 查找还需要继续生成词的句子\n",
    "                cur_indices = [idx for idx, word in enumerate(word_indices)\n",
    "                               if word != self.vocab['<end>']]\n",
    "                if len(cur_indices) > 0:\n",
    "                    cur_sent_indices = sent_indices[cur_indices]\n",
    "                    cur_word_indices = word_indices[cur_indices]\n",
    "                    # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                    cur_sents = cur_sents[cur_indices]\n",
    "                    probs = values[cur_indices].view(-1, 1)\n",
    "                    hidden_state = hidden_state[:, cur_sent_indices, :]\n",
    "                    cur_sent_embed = self.decoder.embed(\n",
    "                        cur_word_indices.view(-1, 1))[:, 0, :]\n",
    "                # 句子太长，停止生成\n",
    "                if cur_sents.size(1) >= max_len:\n",
    "                    break\n",
    "            if len(end_sents) == 0:\n",
    "                # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "                gen_sent = cur_sents[0].tolist()\n",
    "            else:\n",
    "                # 否则选取包含结束符的句子中概率最大的句子\n",
    "                gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "            texts.append(gen_sent)\n",
    "        return texts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab40489",
   "metadata": {},
   "source": [
    "6. 损失函数 `PackedCrossEntropyLoss` 类：为序列学习任务定义了交叉熵损失函数，忽略填充的部分。它使用了 `pack_padded_sequence` 来处理不同长度的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "```Python\n",
    "# 损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        计算交叉熵损失，排除填充的部分。\n",
    "        参数：\n",
    "            predictions：模型的预测结果，形状为 (batch_size, max_length, vocab_size)。\n",
    "            targets：实际的文本描述，形状为 (batch_size, max_length)。\n",
    "            lengths：每个描述的实际长度。\n",
    "        \"\"\"\n",
    "        packed_predictions = pack_padded_sequence(predictions, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "        packed_targets = pack_padded_sequence(targets, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 计算损失，忽略填充的部分\n",
    "        loss = self.loss_fn(packed_predictions, packed_targets)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9819",
   "metadata": {},
   "source": [
    "#### 4.2.4 模型训练\n",
    "\n",
    "`train.py` 文件中的 `main` 函数实现了模型训练的完整流程，包括数据准备、模型初始化、训练循环、损失计算、优化步骤以及模型评估。下面是详细的步骤分析：\n",
    "\n",
    "1. **配置加载**：\n",
    "   - 加载配置参数，这些参数在 `configurations.py` 文件中被定义。\n",
    "\n",
    "2. **数据加载器创建**：\n",
    "   - 使用 `create_dataloaders` 函数创建用于训练和测试的数据加载器。\n",
    "\n",
    "3. **词汇表加载**：\n",
    "   - 加载词汇表文件，这对于后续将文本编码和解码成数字是必要的。\n",
    "\n",
    "4. **模型初始化**：\n",
    "   - 实例化 `ARCTIC` 模型，传入必要的参数，如图像编码维度、词汇表、词嵌入维度等，并将模型转移到配置指定的设备上（如 GPU）。\n",
    "\n",
    "5. **优化器设置**：\n",
    "   - 调用 `get_optimizer` 函数为模型设置优化器，以用于训练中的参数更新。\n",
    "\n",
    "6. **损失函数定义**：\n",
    "   - 实例化 `PackedCrossEntropyLoss` 类，用于计算模型输出和目标序列之间的损失。\n",
    "\n",
    "7. **权重保存路径创建**：\n",
    "   - 创建用于保存训练过程中模型权重的目录。\n",
    "\n",
    "8. **训练循环**：\n",
    "   - 对于设定的训练轮次，执行以下操作：\n",
    "     - 将模型置于训练模式。\n",
    "     - 遍历训练数据加载器中的数据批次，对于每个批次：\n",
    "       - 将图像和文本数据移至配置指定的设备。\n",
    "       - 清空优化器状态。\n",
    "       - 通过模型传递图像和文本，获取输出和注意力权重。\n",
    "       - 计算损失，考虑到序列的实际长度。\n",
    "       - 执行反向传播和优化器步骤以更新权重。\n",
    "       - 定期打印损失信息。\n",
    "\n",
    "9. **模型评估**：\n",
    "   - 在每个训练轮次后，使用测试数据集评估模型性能，并打印 CIDEr 评分。\n",
    "\n",
    "10. **模型保存**：\n",
    "    - 如果当前模型性能好于之前的最佳性能，则保存模型权重（注释中提到的代码被注释掉了，但这是典型的做法）。\n",
    "    - 在训练完成后，保存最终的模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee32cc8",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from configurations import Config\n",
    "from models import ARCTIC, get_optimizer, PackedCrossEntropyLoss, evaluate_cider\n",
    "from datasets import create_dataloaders, ImageTextDataset\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "    with open('../data/output/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    model = ARCTIC(\n",
    "        image_code_dim=config.image_code_dim,\n",
    "        vocab=vocab,  # 传递词汇表字典\n",
    "        word_dim=config.word_dim,\n",
    "        attention_dim=config.attention_dim,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers\n",
    "    ).to(config.device)\n",
    "    \n",
    "    optimizer = get_optimizer(model, config)\n",
    "    loss_fn = PackedCrossEntropyLoss().to(config.device)\n",
    "    \n",
    "    weights_dir = os.path.join(config.output_folder, 'weights')\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "    best_val_score = float('-inf')  # 初始化最佳验证得分\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "            imgs, caps = imgs.to(config.device), caps.to(config.device)\n",
    "            caplens = caplens.cpu().to(torch.int64)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, alphas, _, _, _ = model(imgs, caps, caplens)\n",
    "\n",
    "            # 确保目标序列长度与模型输出匹配\n",
    "            targets = caps[:, 1:]  # 假设targets是captions去除第一个<start>标记后的部分\n",
    "            print(f\"Caplens: {caplens}\")\n",
    "            loss = loss_fn(outputs, targets, caplens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印/记录损失信息\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f'Epoch [{epoch + 1}/{config.num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # 在每个epoch结束时使用测试集评估模型\n",
    "        current_test_score = evaluate_cider(test_loader, model, config)\n",
    "        print(f\"Epoch {epoch}: Test score = {current_test_score}\")\n",
    "\n",
    "    # 训练完成后的最终评估\n",
    "    final_test_score = evaluate_cider(test_loader, model, config)\n",
    "    print(f\"Final test score = {final_test_score}\")\n",
    "\n",
    "    # 训练完成后保存模型\n",
    "    final_model_path = os.path.join(weights_dir, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Saved final model to {final_model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b471d3",
   "metadata": {},
   "source": [
    "### 4.3 Transformer Encoder and Decoder\n",
    "\n",
    "Transformer Model 整体架构图：\n",
    "\n",
    "![image](../doc/img/Transformer_framework.png)\n",
    "\n",
    "我们首先使用 argparse 库解析命令行参数，获取图像路径、模型版本和 Checkpoint 路径；其次根据命令行参数加载预训练模型，或者从 Checkpoint 加载模型（可选）；紧接着使用 PIL 库打开图像，并进行预处理；然后使用模型生成图像的描述；最后使用 METEOR 和 ROUGE-L 评估生成的描述与参考描述的相似度。\n",
    "\n",
    "我们定义了一个名为 `MyDataset` 的类，这个类继承自 PyTorch 的 `Dataset` 基类。在 `init` 方法中，这个类接受一个 json 文件的路径、一个图像目录的路径和一个可选的图像转换函数。json 文件中应该包含图像文件名和对应的标题。这个方法首先读取 json 文件并将其内容保存在 `self.data` 中，然后保存图像目录的路径和图像转换函数。最后，它从 `self.data` 中提取所有的文件名并保存在 `self.filenames` 中。`__len__` 方法返回数据集中的样本数量，这是通过返回 `self.data` 的长度来实现的。`__getitem__` 方法接受一个索引 `idx`，并返回对应的图像和标题。它首先从 `self.filenames` 中获取文件名，然后从 `self.data` 中获取对应的标题。接着，它打开对应的图像文件，并如果提供了图像转换函数，就对图像进行转换。最后，它返回图像和标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c931f",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        caption = self.data[filename]\n",
    "        image = Image.open(f\"{self.img_dir}/{filename}\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54d6d3",
   "metadata": {},
   "source": [
    "#### 4.3.1 参数解析模块\n",
    "\n",
    "解析命令行参数，获取图像路径、模型版本和 Checkpoint 路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d13fb",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Image Captioning')\n",
    "parser.add_argument('--path', type=str, help='path to image',\n",
    "required=True)\n",
    "parser.add_argument('--v', type=str, help='version')\n",
    "parser.add_argument('--checkpoint', type=str, help='checkpoint\n",
    "path', default=None)\n",
    "args = parser.parse_args()\n",
    "image_path = args.path\n",
    "version = args.v\n",
    "checkpoint_path = args.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758591a0",
   "metadata": {},
   "source": [
    "#### 4.3.2 图像预处理模块\n",
    "\n",
    "使用 PIL 库打开图像，并进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92797637",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "start_token = tokenizer.convert_tokens_to_ids(tokenizer._cls_token)\n",
    "end_token = tokenizer.convert_tokens_to_ids(tokenizer._sep_token)\n",
    "image = Image.open(image_path)\n",
    "image = coco.val_transform(image)\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a581a99",
   "metadata": {},
   "source": [
    "#### 4.3.3 Caption 生成模块\n",
    "\n",
    "顾名思义，使用模型生成图像的描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef20e10",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "    return caption_template, mask_template\n",
    "\n",
    "caption, cap_mask = create_caption_and_mask(start_token,\n",
    "                                            config.max_position_embeddings)\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    for i in range(config.max_position_embeddings - 1):\n",
    "        predictions = model(image, caption, cap_mask)\n",
    "        predictions = predictions[:, i, :]\n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "        if predicted_id[0] == 102:\n",
    "\t\t    return caption\n",
    "        caption[:, i+1] = predicted_id[0]\n",
    "        cap_mask[:, i+1] = False\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e45ee",
   "metadata": {},
   "source": [
    "#### 4.3.4 评估模块\n",
    "\n",
    "使用 METEOR 和 ROUGE-L 评估生成的描述与参考描述的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98e304",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def calc_meteor(reference, hypothesis):\n",
    "    hypothesis = word_tokenize(hypothesis)\n",
    "    reference = word_tokenize(reference)\n",
    "    return single_meteor_score(reference, hypothesis)\n",
    "\n",
    "def calc_rouge_l(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return scores[0]['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80676913",
   "metadata": {},
   "source": [
    "## 五、实验结果与分析\n",
    "\n",
    "### 5.1 输出效果展示\n",
    "\n",
    "#### 5.1.1 Attention and Self Attention 输出展示\n",
    "\n",
    "> 待补充\n",
    "\n",
    "#### 5.1.2 Transformer Encoder and Decoder 输出展示（这个数值要调）\n",
    "\n",
    "![image](../doc/img/Transformer_demo1.png)\n",
    "\n",
    "![image](../doc/img/Transformer_demo2.png)\n",
    "\n",
    "#### 5.1.3 BLIP 多模态输出展示\n",
    "\n",
    "![image](../doc/img/BLIP_1.png)\n",
    "\n",
    "![image](../doc/img/BLIP_2.png)\n",
    "\n",
    "![image](../doc/img/BLIP_3.png)\n",
    "\n",
    "### 5.2 评测指标统计\n",
    "\n",
    "> 待补充\n",
    "\n",
    "### 5.3 对比分析\n",
    "\n",
    "> 待补充，可能包括不同模型的对比，误差分析，局限性分析和可能的改进方向。\n",
    ">\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "在进行实验的过程中，我们深刻体会到了许多关于计算机视觉和自然语言处理交叉领域的知识应用，关于经验和心得，可说的包括但不限于以下内容：\n",
    "\n",
    "首先是模型选择，在选择模型结构时，我们意识到了模型的复杂性与性能之间的平衡。过于简单的模型可能无法捕捉到复杂的图像语境，而过于复杂的模型可能导致过拟合，而且可能会对硬件设备的要求比较高，综合考虑之下，我们选择了比较合适的几个模型。\n",
    "\n",
    "其次是数据预处理部分，实验中，我们一开始没有对数据进行预处理，结果得到的效果不尽如人意，后来我们着手进行了数据的预处理，使得模型能够更好地理解输入数据，并提高了训练的效果。\n",
    "\n",
    "紧接着是超参数的调优，通过多次实验，我们认识到超参数的选择和模型微调对实验结果的影响是巨大的。系统地调整学习率、Batch Size 和 Epoch 次数等超参数，结合模型微调，对于提高模型性能起到了关键作用。尤其是 Batch Size，这直接影响到了我们是否能够开始训练，在实验中，我们遇到了一个棘手的问题是内存不足：\n",
    "\n",
    "![image](../doc/img/Out_of_Memory.png)\n",
    "\n",
    "经过不断的查找解决方法，最终我们发现调整 Batch Size 和使用梯度积累的方法可以改善这种情况。\n",
    "\n",
    "然后是评估指标的综合考虑，在评估模型性能时，我采用了多个评估指标，包括 METEOR、ROUGE-L 等。这帮助我更全面地了解了模型生成描述的质量。综合考虑不同指标的结果，有助于更全面地评估模型的性能。部分计算评估指标的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d7a5d",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# 计算METEOR分数\n",
    "def calc_meteor(reference, hypothesis):\n",
    "    hypothesis = word_tokenize(hypothesis)\n",
    "    reference = word_tokenize(reference)\n",
    "    return single_meteor_score(reference, hypothesis)\n",
    "\n",
    "# 计算ROUGE-L分数\n",
    "def calc_rouge_l(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return scores[0]['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650cd35",
   "metadata": {},
   "source": [
    "最后，通过这个实验，我们深感计算机视觉和自然语言处理的快速发展，也认识到了自己的知识不足之处。我们经过这次实验，对深度学习、图像处理和文本生成等领域有了更深入的理解，也激发了我们对未来深入学习和探索的兴趣。如果要用一句话来概括这整个神经网络与深度学习课程设计，我们会说这次实验是我们从理论到实践的一次重要尝试，通过不断调整和优化，我们逐渐提高了对这一复杂任务的理解，同时也加深了对深度学习技术的认识。这次实验不仅是对知识的巩固，也是对实际问题解决能力的锻炼，为我们未来的研究和工作奠定了坚实的基础，受益匪浅。\n",
    "\n",
    "## 七、口头报告大纲\n",
    "\n",
    "1. 任务描述\n",
    "2. 功能和效果演示\n",
    "3. 其他关键特征（创新性能力或表现、存在的问题或遇到的难题等）"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
