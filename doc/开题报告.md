## 题目：基于编解码框架方法的图像描述生成

> 2023 秋季北京邮电大学深度学习与神经网络课程设计

## 一、任务描述

在这个课程设计中，我们的目标是开发一个基于编解码框架的图像描述生成系统。这个系统能够自动为输入的图片生成流畅且关联的自然语言描述。这个任务涉及到计算机视觉和自然语言处理两个领域，需要我们将图像的视觉信息转化为自然语言的文本信息。我们将采用以下的模型结构来实现这个任务：

1. **网格 / 区域表示**：我们首先将输入的图片通过卷积神经网络进行特征提取，得到图片的网格或区域表示。这些表示能够捕捉到图片中的局部信息和全局信息，为后续的描述生成提供基础。
2. **自注意力 + 注意力机制**：我们将使用自注意力机制来处理图片的网格或区域表示。自注意力机制能够计算每个区域与其他区域之间的关系，帮助我们更好地理解图片的内容。同时，我们还将使用注意力机制来关注图片中的重要区域，这样可以使生成的描述更加准确。
3. **Transformer 编码器 + Transformer 解码器**：我们将使用 Transformer 编码器来处理图片的网格或区域表示，得到一个全局的图片表示。然后，我们将使用 Transformer 解码器来生成描述。在解码过程中，我们将使用注意力机制来关注编码器的输出，这样可以使生成的描述更加准确。

我们的任务将分为以下几个步骤：

1. **数据准备**：我们需要收集一些带有描述的图片数据，用于训练和测试我们的模型。
2. **模型训练**：我们将使用上述的模型结构来训练我们的图像描述生成系统。在训练过程中，我们需要不断调整模型的参数，使得生成的描述能够尽可能地接近真实的描述。
3. **模型测试**：我们将使用一些没有见过的图片来测试我们的模型，看看它能否生成准确的描述。
4. **模型优化**：根据测试结果，我们可能需要对模型进行一些优化，以提高其性能。

我们希望通过这个课程设计，能够深入理解编解码框架、自注意力机制、Transformer 模型等先进的深度学习技术，并能够将这些技术应用到实际问题中。

## 二、预期目标

自动为图片生成流畅关联的自然语言描述

## 三、相关工作

首先我们来介绍一下什么是图像描述技术，事实上就是以图像为输入，通过数学模型和计算使计算机输出对应图像的自然语言描述文字，使计算机拥有**看图说话**的能力，是图像处理领域中继图像识别、图像分割和目标跟踪之后的又一新型任务。

在日常生活中，人们可以将图像中的场景、色彩、逻辑关系等低层视觉特征信息自动建立关系，从而感知图像的高层语义信息，但是计算机作为工具只能提取到数字图像的低层数据特征，而无法像人类大脑一样生成高层语义信息，这就是计算机视觉中的**语义鸿沟**问题。图像描述技术的本质就是将计算机提取的图像视觉特征转化为高层语义信息，即解决语义鸿沟问题，使计算机生成与人类大脑理解相近的对图像的文字描述，从而可以对图像进行分类、检索、分析等处理任务。

### 3.1 基于编解码器的方法

随着深度学习技术的不断发展，神经网络在计算机视觉和自然语言处理领域得到了广泛应用。受机器翻译领域中编解码器模型的启发，图像描述可以通过端到端的学习方法直接实现图像和描述句子之间的映射，将图像描述过程转化成为图像到描述的**翻译**过程。基于深度学习的图像描述生成方法大多采用以 CNN-RNN 为基本模型的编解码器框架，CNN 决定了整个模型的图像识别能力，其最后的隐藏层的输出被用作解码器的输入，RNN 是用来读取编码后的图像并生成文本描述的网络模型，下图是一个简单递归神经网络 RNN 和多模态递归神经网络 m-RNN 架构的示意图：

![image](../doc/img/01.png)

### 3.2 基于注意力机制的方法

近年来，注意力机制被广泛应用于计算机视觉领域，其本质是为了解决编解码器在处理固定长度向量时的局限性。注意力机制并不是将输入序列编码成一个固定向量，而是通过增加一个上下文向量来对每个时间步的输入进行解码，以增强图像区域和单词的相关性，从而获取更多的图像语义细节，下图是一个学习单词 / 图像对齐过程的示意图：

![image](../doc/img/02.png)

### 3.3 基于生成对抗网络的方法

生成对抗网络模型中至少有两个模块：生成网络和判别网络.。在训练过程中，生成网络生成尽量真实的数据以**欺骗**判别网络，并且通过判别网络的损失不断进行学习；而判别网络的任务就是区分生成的数据和真实数据。这两个网络通过动态的博弈学习，可以从无标签的数据中学习特征，从而生成新的数据使用生成对抗网络通过控制随机噪声向量来生成多样化的描述，下图展示了用于单句和段落的生成器 G 和用于单句的评估器 E 的结构：

![image](../doc/img/03.png)

### 3.4 基于强化学习的方法

> 此处是介绍，暂时留白

### 3.5 基于密集描述的方法

> 此处是介绍，暂时留白

## 四、技术方案

### 4.1 模型结构

#### 4.1.1 网格 / 区域表示、自注意力 + 注意力

> 此处是介绍，暂时留白

#### 4.1.2 网格 / 区域表示、Transformer 编码器 + Transformer 解码器

网格 / 区域表示和 Transformer 编码器 + Transformer 解码器的方法结合了计算机视觉和自然语言处理两个领域的先进技术，能够生成准确且流畅的图像描述。

> 网格 / 区域表示：

网格 / 区域表示是一种将图像转化为一系列区域或网格的方法。每个区域或网格都包含了图像的一部分信息，可以看作是图像的一个局部表示。这些局部表示可以通过卷积神经网络进行特征提取，得到每个区域或网格的特征向量。这些特征向量能够捕捉到图像的局部信息和全局信息，为后续的描述生成提供基础。

> Transformer 编码器 + Transformer 解码器：

在图像描述生成任务中，我们可以使用 Transformer 编码器来处理图像的网格或区域表示，得到一个全局的图像表示。然后，我们可以使用 Transformer 解码器来生成描述。在解码过程中，我们可以使用注意力机制来关注编码器的输出，这样可以使生成的描述更加准确。

> 大致流程：

1. **特征提取**：首先，我们需要将输入的图片通过卷积神经网络进行特征提取，得到图片的网格或区域表示。这些表示能够捕捉到图片中的局部信息和全局信息。
2. **编码**：然后，我们将使用 Transformer 编码器来处理图片的网格或区域表示，得到一个全局的图片表示。这个全局的图片表示能够捕捉到图片的整体信息。
3. **解码**：接下来，我们将使用 Transformer 解码器来生成描述。在解码过程中，我们将使用注意力机制来关注编码器的输出，这样可以使生成的描述更加准确。
4. **优化**：最后，我们需要通过优化算法来调整模型的参数，使得生成的描述能够尽可能地接近真实的描述。这个过程通常需要多次迭代，每次迭代都会使模型的性能有所提高。

### 4.2 评测标准

#### 4.2.1 METEOR

METEOR（Metric for Evaluation of Translation with Explicit ORdering）用于评估机器翻译质量，也常用于评估图像描述生成的质量。METEOR 考虑了精确度、召回率和对齐的复杂度，它的计算公式如下：

- $METEOR=P_{mean}(1-Penalty)$

其中，$P_{mean}$ 是调和平均精确度和召回率，$Penalty$ 是对齐的复杂度，其公式如下：

- $P_{mean}=\frac{precision×recall}{precision+recall}$
- $Penalty=γ(\frac{chunks}{matches})^β$

$precision$ 是匹配的单词数占生成描述的单词数的比例，$recall$ 是匹配的单词数占真实描述的单词数的比例，$chunks$ 是连续匹配的块数，$matches$ 是匹配的单词数，而 $β$ 和 $γ$ 是超参数。

#### 4.2.2 ROUGE-L

ROUGE-L（Recall-Oriented Understudy for Gisting Evaluation with Longest Common Subsequence）是基于最长公共子序列的评估指标，也常用于评估图像描述生成的质量。ROUGE-L 考虑了生成描述和真实描述之间的最长公共子序列，它的计算公式如下：

- $ROUGE-L=\frac{LCS(X,Y)}{max(|X|,|Y|)}$

其中，$X$ 和 $Y$ 分别是生成描述和真实描述，$∣X∣$  和 $∣Y∣$  分别是生成描述和真实描述的长度，$LCS(X, Y)$ 是生成描述和真实描述之间的最长公共子序列的长度。

#### 4.2.3 CIDEr-D

> 此处是介绍，暂时留白

### 4.3 其他内容

#### 4.3.1 实现基于强化学习的损失函数，直接优化评测指标

> 此处是介绍，暂时留白

#### 4.3.2 微调多模态预训练模型或多模态大语言模型，并测试性能

> 此处是介绍，暂时留白

## 五、开发环境

- Linux Ubuntu 20.04 / 22.04
- NVIDIA GPU
- CUDA 12.2

## 六、组员分工及检查点

> 分工表：

|                            巩羽飞                            |                       黄成梓                       |
| :----------------------------------------------------------: | :------------------------------------------------: |
| 模型：网格 / 区域表示、Transformer 编码器 + Transformer 解码器 |      模型：网格 / 区域表示、自注意力 + 注意力      |
|                    指标：METEOR + ROUGE-L                    |                   指标：CIDEr-D                    |
|   其他：微调多模态预训练模型或多模态大语言模型，并测试性能   | 其他：实现基于强化学习的损失函数，直接优化评测指标 |

> 检查点：

|        | Point 1  |          Point 2          |     Point 3      |         Point 4          |
| ------ | :------: | :-----------------------: | :--------------: | :----------------------: |
| 巩羽飞 | 开题报告 | 跑通负责的模型 + 评测指标 |     中期报告     | 附加任务微调多模态大模型 |
| 黄成梓 | 开题报告 | 跑通负责的模型 + 评测指标 | 附加任务优化指标 |         结题报告         |

## 七、时间安排

|  11.25   |        11.30        |            12.12            |                12.28                |
| :------: | :-----------------: | :-------------------------: | :---------------------------------: |
| 开题报告 | 模型跑通 + 评测指标 | 中期报告 + 附加任务优化指标 | 结题报告 + 附加任务微调多模态大模型 |

## 八、参考文献

[1] Mao， Junhua et al. “Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).” *arXiv: Computer Vision and Pattern Recognition* (2014): n. pag.

[2] Xu， Ke et al. “Show， Attend and Tell: Neural Image Caption Generation with Visual Attention.” *International Conference on Machine Learning* (2015).

[3] Dai, Bo et al. “Towards Diverse and Natural Image Descriptions via a Conditional GAN.” *2017 IEEE International Conference on Computer Vision (ICCV)* (2017): 2989-2998.