{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 利用网格/区域表示+自注意力+注意力的模型结构来完成副使图像描述任务"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83a4b24c4062fa1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 参数配置\n",
    "`configurations.py` 文件中定义的 `Config` 类作为项目的配置中心，其作用是集中管理项目中使用的所有配置参数。这些参数通常包括文件路径、模型参数、数据处理选项、训练设置和图像处理参数等。通过这种方式，可以在不修改代码的情况下调整项目的行为。\n",
    "\n",
    "以下是`Config`类中定义的配置参数及其作用：\n",
    "\n",
    "1. **数据路径**：\n",
    "   - `data_path`：主数据目录路径。\n",
    "   - `images_path`：存储图像的路径。\n",
    "   - `train_captions_path`：训练集的文本描述文件路径。\n",
    "   - `test_captions_path`：测试集的文本描述文件路径。\n",
    "   - `output_folder`：用于存储词汇表和处理后数据的输出文件夹路径。\n",
    "\n",
    "2. **模型参数**：\n",
    "   - `embed_size`：嵌入向量的维度。\n",
    "   - `vocab_size`：词汇表的大小。\n",
    "   - `num_layers`：定义循环神经网络中的层数。\n",
    "   - `num_heads`：自注意力机制中头的数量。\n",
    "   - `dropout`：在模型中使用的dropout比率。\n",
    "   - `hidden_size`：隐藏层的维度。\n",
    "   - `image_code_dim`：图像编码的维度。\n",
    "   - `word_dim`：词嵌入的维度。\n",
    "   - `attention_dim`：注意力机制的隐藏层维度。\n",
    "\n",
    "3. **数据处理参数**：\n",
    "   - `min_word_count`：词汇表中词的最小出现次数，用于筛选词汇。\n",
    "   - `max_len`：假设的描述的最大长度。\n",
    "\n",
    "4. **训练参数**：\n",
    "   - `batch_size`：每个批次的大小。\n",
    "   - `learning_rate`：学习率。\n",
    "   - `num_epochs`：训练的总轮次数。\n",
    "   - `workers`：加载数据时使用的工作线程数。\n",
    "   - `encoder_learning_rate`：编码器的学习率。\n",
    "   - `decoder_learning_rate`：解码器的学习率。\n",
    "   - `lr_update`：学习率更新频率。\n",
    "\n",
    "5. **图像预处理参数**：\n",
    "   - `image_size`：图像缩放后的大小。\n",
    "   - `crop_size`：从缩放后的图像中裁剪出的大小。\n",
    "\n",
    "6. **其他配置**：\n",
    "   - `device`：设置运行计算的设备，如果CUDA可用则使用GPU，否则使用CPU。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46d1df844e1e0eba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理\n",
    "为图像描述任务准备和预处理数据，确保数据能够被模型以适当的格式接受和处理。它是建立有效的训练和测试环境的基础。\n",
    "\n",
    "这个`datasets.py`文件实现了以下几个主要功能：\n",
    "\n",
    "1. `create_dataset`函数：用于处理原始文本描述，创建一个词汇表，并将文本转换为对应的词索引向量。它首先读取训练和测试数据集中的文本描述，然后统计词频以创建词汇表，并移除低频词。之后，它定义了一个内部函数`encode_captions`，这个函数负责将每条文本描述转换为一个固定长度的词索引序列，包括特殊标记<start>, <end>, <pad>, 和<unk>。转换完成后，函数将这些数据保存为JSON文件，以便后续处理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c9dda088a79cb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from configurations import Config  # 导入配置类\n",
    "\n",
    "\n",
    "# 从配置文件获取配置\n",
    "config = Config()\n",
    "\n",
    "\n",
    "def create_dataset(max_len=64):\n",
    "    \"\"\"\n",
    "    整理数据集，构建词汇表，并将文本描述转换为词索引向量。\n",
    "    使用configuration.py文件中定义的配置信息。\n",
    "    \"\"\"\n",
    "    # 使用config中定义的路径\n",
    "    image_folder = config.images_path\n",
    "    train_captions_path = config.train_captions_path\n",
    "    test_captions_path = config.test_captions_path\n",
    "    output_folder = config.output_folder\n",
    "\n",
    "    # 读取训练图像描述\n",
    "    with open(train_captions_path, 'r') as f:\n",
    "        train_captions_data = json.load(f)\n",
    "\n",
    "    # 读取测试图像描述\n",
    "    with open(test_captions_path, 'r') as f:\n",
    "        test_captions_data = json.load(f)\n",
    "\n",
    "    # 统计训练集的文本描述的词频\n",
    "    vocab = Counter()\n",
    "    for caption in train_captions_data.values():\n",
    "        vocab.update(caption.lower().split())\n",
    "\n",
    "    # 移除其中的低频词\n",
    "    vocab = {word for word, count in vocab.items() if count >= config.min_word_count}\n",
    "\n",
    "    # 构建词典\n",
    "    word_to_idx = {word: idx + 4 for idx, word in enumerate(vocab)}\n",
    "    word_to_idx['<pad>'] = 0\n",
    "    word_to_idx['<start>'] = 1\n",
    "    word_to_idx['<end>'] = 2\n",
    "    word_to_idx['<unk>'] = 3\n",
    "\n",
    "    # 一个函数来转换描述为词索引向量，并进行填充\n",
    "    def encode_captions(captions_data, word_to_idx, max_len):\n",
    "        encoded_captions = {}\n",
    "        caplens = {}\n",
    "        for img_id, caption in captions_data.items():\n",
    "            words = caption.lower().split()\n",
    "            encoded_caption = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "            caplen = len(encoded_caption) + 2  # 加2是因为还要加上<start>和<end>\n",
    "            encoded_caption = [word_to_idx['<start>']] + encoded_caption + [word_to_idx['<end>']]\n",
    "            encoded_caption += [word_to_idx['<pad>']] * (max_len - len(encoded_caption))\n",
    "            encoded_captions[img_id] = encoded_caption[:max_len]\n",
    "            caplens[img_id] = caplen if caplen <= max_len else max_len\n",
    "        return encoded_captions, caplens\n",
    "    # def encode_captions(captions_data, word_to_idx, max_len):\n",
    "    #     encoded_captions = {}\n",
    "    #     for img_id, caption in captions_data.items():\n",
    "    #         words = caption.lower().split()\n",
    "    #         encoded_caption = [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
    "    #         encoded_caption = [word_to_idx['<start>']] + encoded_caption + [word_to_idx['<end>']]\n",
    "    #         encoded_caption += [word_to_idx['<pad>']] * (max_len - len(encoded_caption))\n",
    "    #         encoded_captions[img_id] = encoded_caption[:max_len]\n",
    "    #     return encoded_captions\n",
    "\n",
    "    # 对训练集描述进行编码\n",
    "    encoded_captions_train, caplens_train = encode_captions(train_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 对测试集描述进行编码\n",
    "    encoded_captions_test, caplens_test = encode_captions(test_captions_data, word_to_idx, max_len)\n",
    "\n",
    "    # 存储词典和编码后的描述\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as f:\n",
    "        json.dump(word_to_idx, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_train.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'encoded_captions_test.json'), 'w') as f:\n",
    "        json.dump(encoded_captions_test, f)\n",
    "\n",
    "    # 存储图像路径\n",
    "    image_paths_train = {img_id: os.path.join(image_folder, img_id) for img_id in train_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_train.json'), 'w') as f:\n",
    "        json.dump(image_paths_train, f)\n",
    "\n",
    "    image_paths_test = {img_id: os.path.join(image_folder, img_id) for img_id in test_captions_data.keys()}\n",
    "    with open(os.path.join(output_folder, 'image_paths_test.json'), 'w') as f:\n",
    "        json.dump(image_paths_test, f)\n",
    "\n",
    "    # 存储caplens\n",
    "    with open(os.path.join(output_folder, 'caplens_train.json'), 'w') as f:\n",
    "        json.dump(caplens_train, f)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'caplens_test.json'), 'w') as f:\n",
    "        json.dump(caplens_test, f)\n",
    "\n",
    "\n",
    "# 调用函数，整理数据集\n",
    "# create_dataset()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88397a8d006251c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. `ImageTextDataset`类：继承自`torch.utils.data.Dataset`，这个类是一个PyTorch的自定义数据集，用于加载图像和对应的已编码文本描述。它重写了`__getitem__`方法，用于获取索引对应的数据点（图像和文本描述），并将图像通过转换处理成统一的格式；重写了`__len__`方法，返回数据集的大小。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f57e80965890820"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch数据集类，用于加载和处理图像-文本数据。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths_file, captions_file, caplens_file, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集类。\n",
    "        参数:\n",
    "            image_paths_file: 包含图像路径的json文件路径。\n",
    "            captions_file: 包含编码后文本描述的json文件路径。\n",
    "            transform: 应用于图像的预处理转换。\n",
    "        \"\"\"\n",
    "        # 载入图像路径和文本描述以及caplens\n",
    "        with open(image_paths_file, 'r') as f:\n",
    "            self.image_paths = json.load(f)\n",
    "\n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "        with open(caplens_file, 'r') as f:\n",
    "            self.caplens = json.load(f)\n",
    "\n",
    "        # 设置图像预处理方法\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取单个数据点。\n",
    "        参数:\n",
    "            index: 数据点的索引。\n",
    "        返回:\n",
    "            一个包含图像和对应文本描述的元组。\n",
    "        \"\"\"\n",
    "        # 获取图像路径和文本描述以及caplen\n",
    "        image_id = list(self.image_paths.keys())[index]\n",
    "        image_path = self.image_paths[image_id]\n",
    "        caption = self.captions[image_id]\n",
    "        caplen = self.caplens[image_id]\n",
    "\n",
    "        # 加载图像并应用预处理\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 将文本描述转换为张量\n",
    "        caption_tensor = torch.tensor(caption, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tensor, caplen\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        数据集中的数据点总数。\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "# 创建数据集实例\n",
    "# train_dataset = ImageTextDataset(\n",
    "#     image_paths_file=os.path.join(config.output_folder, 'image_paths_train.json'),\n",
    "#     captions_file=os.path.join(config.output_folder, 'encoded_captions_train.json'),\n",
    "#     caplens_file=os.path.join(config.output_folder, 'caplens_train.json')\n",
    "# )\n",
    "#\n",
    "# # 示例：创建验证集实例\n",
    "# test_dataset = ImageTextDataset(\n",
    "#     image_paths_file=os.path.join(config.output_folder, 'image_paths_test.json'),\n",
    "#     captions_file=os.path.join(config.output_folder, 'encoded_captions_test.json'),\n",
    "#     caplens_file=os.path.join(config.output_folder, 'caplens_test.json')\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "633b41c597ce9d84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "3. `create_dataloaders`函数：使用`ImageTextDataset`类来创建PyTorch的`DataLoader`，它提供了一个可迭代的数据加载器，用于在训练和测试时批量加载数据，并可选地对数据进行打乱和多进程加载。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec6e5462409b93d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 创建训练集和测试集的 DataLoader\n",
    "def create_dataloaders(config):\n",
    "    \"\"\"\n",
    "    创建训练集和测试集的 DataLoader。\n",
    "\n",
    "    参数:\n",
    "        batch_size: 每个批次的大小。\n",
    "        num_workers: 加载数据时使用的进程数。\n",
    "        shuffle_train: 是否打乱训练数据。\n",
    "\n",
    "    返回:\n",
    "        train_loader: 训练数据的 DataLoader。\n",
    "        test_loader: 测试数据的 DataLoader。\n",
    "    \"\"\"\n",
    "    # 图像预处理转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 加载数据时使用的进程数\n",
    "    num_workers = 0\n",
    "\n",
    "    # 创建数据集对象\n",
    "    train_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_train.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_train.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_train.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = ImageTextDataset(\n",
    "        image_paths_file=os.path.join(config.output_folder, 'image_paths_test.json'),\n",
    "        captions_file=os.path.join(config.output_folder, 'encoded_captions_test.json'),\n",
    "        caplens_file=os.path.join(config.output_folder, 'caplens_test.json'),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # 创建 DataLoader 对象\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,  # 通常测试集不需要打乱\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad067173fc942d67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "此外，在文件的末尾，通过`if __name__ == '__main__':`块，文件提供了一个简单的测试用例，用于验证训练数据加载器是否正确创建，并能够生成批量数据。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66267e2ec1337ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = Config()\n",
    "# 使用Config类中定义的配置来创建DataLoader\n",
    "train_loader, test_loader = create_dataloaders(config=config)\n",
    "\n",
    "# 测试 DataLoader 是否正确创建\n",
    "if __name__ == '__main__':\n",
    "    for i, (images, captions, caplens) in enumerate(train_loader):\n",
    "        print(f\"Batch {i + 1}\")\n",
    "        print(f\"Images shape: {images.size()}\")\n",
    "        print(f\"Captions shape: {captions.size()}\")\n",
    "        if i == 1:  # 仅打印前两个批次的信息\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43dc9a14441203c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我还定义了一个 `datasets_pretrain_demo.py` 文件来验证数据预处理过程是否正确。它通过以下步骤实现这一目标：\n",
    "\n",
    "1. **读取词汇表和编码后的描述**：\n",
    "   - 加载之前生成的词汇表 (`vocab.json`)，编码后的训练集描述 (`encoded_captions_train.json`)，以及训练图像的路径 (`image_paths_train.json`)。\n",
    "\n",
    "2. **索引到单词的转换**：\n",
    "   - 创建从词索引到单词的反向映射，用于将编码后的描述转换回文本形式。\n",
    "\n",
    "3. **选择并展示图像**：\n",
    "   - 从图像路径列表中选择第一个图像ID，并加载对应的图像。\n",
    "\n",
    "4. **展示图像**：\n",
    "   - 使用matplotlib展示图像，并关闭坐标轴。\n",
    "\n",
    "5. **打印文本描述**：\n",
    "   - 将编码后的描述（词索引列表）转换回单词形式，并打印出来，以验证编码和图像加载的正确性。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c2da7f2755a708"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_path = '../data/output/vocab.json'\n",
    "encoded_captions_path = '../data/output/encoded_captions_train.json'\n",
    "image_paths_path = '../data/output/image_paths_train.json'\n",
    "\n",
    "# 读取词典、编码后的描述和图像路径\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open(encoded_captions_path, 'r') as f:\n",
    "    encoded_captions = json.load(f)\n",
    "\n",
    "with open(image_paths_path, 'r') as f:\n",
    "    image_paths = json.load(f)\n",
    "\n",
    "# 将索引转换回单词\n",
    "vocab_idx2word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# 选择要展示的图片ID，这里以第一个ID为例\n",
    "first_img_id = list(image_paths.keys())[0]\n",
    "content_img = Image.open(image_paths[first_img_id])\n",
    "\n",
    "# 展示图片和对应的描述\n",
    "plt.imshow(content_img)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "# 打印对应的文本描述，确保字典中的键是整数，直接使用整数索引\n",
    "caption = ' '.join([vocab_idx2word[word_idx] for word_idx in encoded_captions[first_img_id]])\n",
    "# caption = ' '.join([vocab_idx2word[str(word_idx)] for word_idx in encoded_captions[first_img_id]])\n",
    "print(caption)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dba5f8192fc89d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型定义\n",
    "`models.py`文件定义了用于服饰图像描述任务的神经网络模型，包括图像编码器、注意力机制、文本解码器、整体模型框架和损失函数。以下是代码中各个部分的详细作用：\n",
    "\n",
    "1. **自注意力机制 (`SelfAttention` 类)**：\n",
    "   - 定义了一个利用`nn.MultiheadAttention`实现的自注意力层。它可以处理图像的特征，使模型能够在图像的不同区域之间建立联系，这在解析复杂图像时非常有用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b11e65f5cefc0206"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import numpy as np\n",
    "from configurations import Config\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "# 引入自注意机制后的图像编码器\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_channels, num_heads=8, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = nn.MultiheadAttention(num_channels, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 保存原始形状\n",
    "        orig_shape = x.shape\n",
    "        # 打印输入形状\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        # 转换为(sequence_length, batch_size, num_channels)格式\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        # 还原形状，确保与原始输入形状匹配\n",
    "        attention_output = attention_output.permute(1, 2, 0)# 打印最终输出形状\n",
    "        print(\"Final output shape:\", attention_output.shape)\n",
    "        return attention_output.view(orig_shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f157dfa5c9cacc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. **图像编码器 (`ImageEncoder` 类)**：\n",
    "   - 使用预训练的ResNet-101模型作为特征提取器，抽取图像的高层特征。这些特征接着被自注意力层进一步处理，以增强图像区域间的相关性。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a895a0297162c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True, num_heads=8, dropout=0.1):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # 使用ResNet101作为基础模型\n",
    "        model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        # 设置参数是否可训练\n",
    "        for param in self.grid_rep_extractor.parameters():\n",
    "            param.requires_grad = finetuned\n",
    "\n",
    "        # 自注意力层\n",
    "        self.self_attention = SelfAttention(model.fc.in_features, num_heads, dropout)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 通过ResNet网格表示提取器\n",
    "        features = self.grid_rep_extractor(images)\n",
    "        print(\"Extractor output shape:\", features.shape)\n",
    "        # 应用自注意力\n",
    "        features = self.self_attention(features)\n",
    "        # 打印自注意力输出形状\n",
    "        print(\"Self-attention output shape:\", features.shape)\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87eb5ed8d13f58d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. **解码器的注意力机制 (`AdditiveAttention` 类)**：\n",
    "   - 实现了一种加法（或称为Bahdanau）注意力机制，用于计算解码过程中的上下文向量。它通过比较解码器的隐藏状态（query）与图像编码（key-value）之间的关系来计算每个位置的注意力权重。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c1718880c53263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 解码器的注意力机制\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim)\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim)\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        # （2）计算query和key的相关性，实现注意力评分函数\n",
    "        # -> (batch_size, 1, attn_dim)\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1)\n",
    "        # -> (batch_size, n_kv, attn_dim)\n",
    "        keys = self.attn_w_1_k(key_value)\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2)\n",
    "        # （3）归一化相关性分数\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.softmax(attn)\n",
    "        # （4）计算输出\n",
    "        # (batch_size x 1 x n_kv)(batch_size x n_kv x kv_dim)\n",
    "        # -> (batch_size, 1, kv_dim)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7414add688e0f6f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. **文本解码器 (`AttentionDecoder` 类)**：\n",
    "   - 定义了一个注意力机制的解码器，它结合了图像编码和前一个时间步的词嵌入来生成文本描述。解码器使用GRU单元进行序列生成，并且在每个时间步使用注意力权重来关注图像的不同区域。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a304415e0faa992a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 文本解码器\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "           初始化文本解码器。\n",
    "\n",
    "           参数:\n",
    "               image_code_dim: 图像编码的维度。\n",
    "               vocab_size: 词汇表的大小。\n",
    "               word_dim: 词嵌入的维度。\n",
    "               attention_dim: 注意力机制的隐藏层维度。\n",
    "               hidden_size: GRU隐藏层的大小。\n",
    "               num_layers: GRU层数。\n",
    "               dropout: Dropout层的概率。\n",
    "    \"\"\"\n",
    "    def __init__(self, image_code_dim, vocab_size, word_dim, attention_dim, hidden_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, word_dim)\n",
    "        self.attention = AdditiveAttention(hidden_size, image_code_dim, attention_dim)\n",
    "        self.init_state = nn.Linear(image_code_dim, num_layers * hidden_size)\n",
    "        self.rnn = nn.GRU(word_dim + image_code_dim, hidden_size, num_layers)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        # RNN默认已初始化\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        初始化隐藏状态。\n",
    "\n",
    "        参数：\n",
    "            image_code：图像编码器输出的图像表示\n",
    "                        (batch_size, image_code_dim, grid_height, grid_width)\n",
    "            captions: 文本描述。\n",
    "            cap_lens: 文本描述的长度。\n",
    "        \"\"\"\n",
    "        # 将图像网格表示转换为序列表示形式\n",
    "        batch_size, image_code_dim = image_code.size(0), image_code.size(1)\n",
    "        # -> (batch_size, grid_height, grid_width, image_code_dim)\n",
    "        image_code = image_code.permute(0, 2, 3, 1)\n",
    "        # -> (batch_size, grid_height * grid_width, image_code_dim)\n",
    "        image_code = image_code.view(batch_size, -1, image_code_dim)\n",
    "        # （1）按照caption的长短排序\n",
    "        sorted_cap_lens, sorted_cap_indices = torch.sort(cap_lens, 0, True)\n",
    "        captions = captions[sorted_cap_indices]\n",
    "        image_code = image_code[sorted_cap_indices]\n",
    "        # （2）初始化隐状态\n",
    "        hidden_state = self.init_state(image_code.mean(axis=1))\n",
    "        hidden_state = hidden_state.view(\n",
    "            batch_size,\n",
    "            self.rnn.num_layers,\n",
    "            self.rnn.hidden_size).permute(1, 0, 2)\n",
    "        return image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state\n",
    "\n",
    "    def forward_step(self, image_code, curr_cap_embed, hidden_state):\n",
    "        \"\"\"\n",
    "                解码器的前馈步骤。\n",
    "\n",
    "                参数:\n",
    "                    image_code: 图像编码。\n",
    "                    curr_cap_embed: 当前时间步的词嵌入向量。\n",
    "                    hidden_state: 当前的隐藏状态。\n",
    "                \"\"\"\n",
    "        # （3.2）利用注意力机制获得上下文向量\n",
    "        # query：hidden_state[-1]，即最后一个隐藏层输出 (batch_size, hidden_size)\n",
    "        # context: (batch_size, hidden_size)\n",
    "        context, alpha = self.attention(hidden_state[-1], image_code)\n",
    "        # （3.3）以上下文向量和当前时刻词表示为输入，获得GRU输出\n",
    "        x = torch.cat((context, curr_cap_embed), dim=-1).unsqueeze(0)\n",
    "        # x: (1, real_batch_size, hidden_size+word_dim)\n",
    "        # out: (1, real_batch_size, hidden_size)\n",
    "        out, hidden_state = self.rnn(x, hidden_state)\n",
    "        # （3.4）获取该时刻的预测结果\n",
    "        # (real_batch_size, vocab_size)\n",
    "        preds = self.fc(self.dropout(out.squeeze(0)))\n",
    "        return preds, alpha, hidden_state\n",
    "\n",
    "    def forward(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        完整的前馈过程。\n",
    "\n",
    "        参数：\n",
    "            hidden_state: (num_layers, batch_size, hidden_size)\n",
    "            image_code:  (batch_size, feature_channel, feature_size)\n",
    "            captions: (batch_size, )\n",
    "        \"\"\"\n",
    "        # （1）将图文数据按照文本的实际长度从长到短排序\n",
    "        # （2）获得GRU的初始隐状态\n",
    "        image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state \\\n",
    "            = self.init_hidden_state(image_code, captions, cap_lens)\n",
    "        batch_size = image_code.size(0)\n",
    "        # 输入序列长度减1，因为最后一个时刻不需要预测下一个词\n",
    "        lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "        # 初始化变量：模型的预测结果和注意力分数\n",
    "        predictions = torch.zeros(batch_size, lengths[0], self.fc.out_features).to(captions.device)\n",
    "        alphas = torch.zeros(batch_size, lengths[0], image_code.shape[1]).to(captions.device)\n",
    "        # 获取文本嵌入表示 cap_embeds: (batch_size, num_steps, word_dim)\n",
    "        cap_embeds = self.embed(captions)\n",
    "        # Teacher-Forcing模式\n",
    "        for step in range(lengths[0]):\n",
    "            # （3）解码\n",
    "            # （3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths > step)[0].shape[0]\n",
    "            preds, alpha, hidden_state = self.forward_step(\n",
    "                image_code[:real_batch_size],\n",
    "                cap_embeds[:real_batch_size, step, :],\n",
    "                hidden_state[:, :real_batch_size, :].contiguous())\n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "            alphas[:real_batch_size, step, :] = alpha\n",
    "        return predictions, alphas, captions, lengths, sorted_cap_indices\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "969c9481d3ab6a04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. **整体模型 (`ARCTIC` 类)**：\n",
    "   - 将图像编码器和文本解码器整合在一起，定义了完整的模型流程。在前向传递过程中，模型接受图像和文本描述，利用编码器和解码器生成描述的输出。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "539a3af350f652e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ARCTIC 模型\n",
    "class ARCTIC(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, attention_dim, hidden_size, num_layers):\n",
    "        super(ARCTIC, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.decoder = AttentionDecoder(image_code_dim, len(vocab), word_dim, attention_dim, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions, cap_lens):\n",
    "        # 打印图像输入形状\n",
    "        print(\"Image input shape:\", images.shape)\n",
    "        image_code = self.encoder(images)\n",
    "        # 打印编码器输出形状\n",
    "        print(\"Encoder output shape:\", image_code.shape)\n",
    "        output = self.decoder(image_code, captions, cap_lens)\n",
    "        # 打印解码器输出形状\n",
    "        print(\"Decoder output shape:\", output[0].shape)  # Assuming output[0] is the main output\n",
    "        return output\n",
    "\n",
    "    def generate_by_beamsearch(self, images, beam_k, max_len):\n",
    "        vocab_size = len(self.vocab)\n",
    "        image_codes = self.encoder(images)\n",
    "        texts = []\n",
    "        device = images.device\n",
    "        # 对每个图像样本执行束搜索\n",
    "        for image_code in image_codes:\n",
    "            # 将图像表示复制k份\n",
    "            image_code = image_code.unsqueeze(0).repeat(beam_k, 1, 1, 1)\n",
    "            # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "            cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)\n",
    "            cur_sent_embed = self.decoder.embed(cur_sents)[:, 0, :]\n",
    "            sent_lens = torch.LongTensor([1] * beam_k).to(device)\n",
    "            # 获得GRU的初始隐状态\n",
    "            image_code, cur_sent_embed, _, _, hidden_state = \\\n",
    "                self.decoder.init_hidden_state(image_code, cur_sent_embed, sent_lens)\n",
    "            # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "            end_sents = []\n",
    "            # 存储已生成完整的句子的概率\n",
    "            end_probs = []\n",
    "            # 存储未完整生成的句子的概率\n",
    "            probs = torch.zeros(beam_k, 1).to(device)\n",
    "            k = beam_k\n",
    "            while True:\n",
    "                preds, _, hidden_state = self.decoder.forward_step(image_code[:k], cur_sent_embed,\n",
    "                                                                   hidden_state.contiguous())\n",
    "                # -> (k, vocab_size)\n",
    "                preds = nn.functional.log_softmax(preds, dim=1)\n",
    "                # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "                # -> (k, vocab_size)\n",
    "                probs = probs.repeat(1, preds.size(1)) + preds\n",
    "                if cur_sents.size(1) == 1:\n",
    "                    # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                    values, indices = probs[0].topk(k, 0, True, True)\n",
    "                else:\n",
    "                    # probs: (k, vocab_size) 是二维张量\n",
    "                    # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                    # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                    values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "                # 计算最大的k个值对应的句子索引和词索引\n",
    "                sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc')\n",
    "                word_indices = indices % vocab_size\n",
    "                # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "                cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "                # 查找此轮生成句子结束符<end>的句子\n",
    "                end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "                if len(end_indices) > 0:\n",
    "                    end_probs.extend(values[end_indices])\n",
    "                    end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                    # 如果所有的句子都包含结束符，则停止生成\n",
    "                    k -= len(end_indices)\n",
    "                    if k == 0:\n",
    "                        break\n",
    "                # 查找还需要继续生成词的句子\n",
    "                cur_indices = [idx for idx, word in enumerate(word_indices)\n",
    "                               if word != self.vocab['<end>']]\n",
    "                if len(cur_indices) > 0:\n",
    "                    cur_sent_indices = sent_indices[cur_indices]\n",
    "                    cur_word_indices = word_indices[cur_indices]\n",
    "                    # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                    cur_sents = cur_sents[cur_indices]\n",
    "                    probs = values[cur_indices].view(-1, 1)\n",
    "                    hidden_state = hidden_state[:, cur_sent_indices, :]\n",
    "                    cur_sent_embed = self.decoder.embed(\n",
    "                        cur_word_indices.view(-1, 1))[:, 0, :]\n",
    "                # 句子太长，停止生成\n",
    "                if cur_sents.size(1) >= max_len:\n",
    "                    break\n",
    "            if len(end_sents) == 0:\n",
    "                # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "                gen_sent = cur_sents[0].tolist()\n",
    "            else:\n",
    "                # 否则选取包含结束符的句子中概率最大的句子\n",
    "                gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "            texts.append(gen_sent)\n",
    "        return texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e91cc196a7c0bc12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. **损失函数 (`PackedCrossEntropyLoss` 类)**：\n",
    "   - 为序列学习任务定义了交叉熵损失函数，忽略填充的部分。它使用了`pack_padded_sequence`来处理不同长度的序列。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53df808fb2d9e23c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        计算交叉熵损失，排除填充的部分。\n",
    "        参数：\n",
    "            predictions：模型的预测结果，形状为 (batch_size, max_length, vocab_size)。\n",
    "            targets：实际的文本描述，形状为 (batch_size, max_length)。\n",
    "            lengths：每个描述的实际长度。\n",
    "        \"\"\"\n",
    "        # 使用 pack_padded_sequence 来处理变长序列\n",
    "        # 这里 predictions 和 targets 都需要进行 pack 操作\n",
    "        # 由于 pack_padded_sequence 需要长度从长到短的序列，这里假设输入已经是这种格式\n",
    "        packed_predictions = pack_padded_sequence(predictions, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "        packed_targets = pack_padded_sequence(targets, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "\n",
    "        # 计算损失，忽略填充的部分\n",
    "        loss = self.loss_fn(packed_predictions, packed_targets)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f64a9f5e925f9f9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. **优化器 (`get_optimizer` 函数)**：\n",
    "   - 定义了一个函数来为模型的不同部分设置不同的学习率，并创建优化器。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277327489d1ff278"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    \"\"\"\n",
    "    获取优化器，为模型的不同部分设置不同的学习速率。\n",
    "    参数：\n",
    "        model：训练模型。\n",
    "        config：包含配置信息的对象，如学习速率等。\n",
    "    返回：\n",
    "        配置好地优化器。\n",
    "    \"\"\"\n",
    "    # 为编码器和解码器设置不同的学习速率\n",
    "    encoder_params = filter(lambda p: p.requires_grad, model.encoder.parameters())\n",
    "    decoder_params = filter(lambda p: p.requires_grad, model.decoder.parameters())\n",
    "\n",
    "    # 创建优化器，分别对这两部分参数应用不同的学习速率\n",
    "    optimizer = optim.Adam([\n",
    "        {\"params\": encoder_params, \"lr\": config.encoder_learning_rate},\n",
    "        {\"params\": decoder_params, \"lr\": config.decoder_learning_rate}\n",
    "    ])\n",
    "\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5be0769aa8b78f9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. **学习率调整 (`adjust_learning_rate` 函数)**：\n",
    "   - 定义了一个函数来调整优化器中的学习率，根据预设的策略在训练过程中逐渐减小学习率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ae0176024d82bb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"\n",
    "    调整学习速率，每隔一定轮次减少到原来的十分之一。\n",
    "    参数：\n",
    "        optimizer：优化器。\n",
    "        epoch：当前轮次。\n",
    "        config：包含配置信息的对象。\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['name'] == 'encoder':\n",
    "            param_group['lr'] = config.encoder_learning_rate * (0.1 ** (epoch // config.lr_update))\n",
    "        else:\n",
    "            param_group['lr'] = config.decoder_learning_rate * (0.1 ** (epoch // config.lr_update))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "911d998168b52ad7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. **评估函数 (`evaluate_cider` 函数)**：\n",
    "   - 定义了一个函数来评估生成的描述的质量，使用CIDEr-D评分系统对模型性能进行评估。这个评分系统特别关注描述的差异性。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1782228080b00a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CIDEr-D 评估\n",
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与CIDEr-D计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "\n",
    "\n",
    "def evaluate_cider(data_loader, model, config):\n",
    "    model.eval()\n",
    "    # 存储候选文本和参考文本\n",
    "    cands = {}\n",
    "    refs = {}\n",
    "    filterd_words = {model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']}\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, (imgs, caps, caplens, allcaps) in enumerate(data_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        # Generate captions\n",
    "        preds = model.sample(imgs)\n",
    "        for j in range(imgs.size(0)):\n",
    "            img_id = str(i * config.batch_size + j)\n",
    "            cand = ' '.join(filter_useless_words(preds[j], filterd_words))\n",
    "            cands[img_id] = [cand]\n",
    "            refs[img_id] = list(map(lambda x: ' '.join(filter_useless_words(x, filterd_words)), allcaps[j].tolist()))\n",
    "\n",
    "    # 计算CIDEr-D得分\n",
    "    cider_evaluator = Cider()\n",
    "    score, scores = cider_evaluator.compute_score(refs, cands)\n",
    "\n",
    "    model.train()\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a831c1103c1adab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型训练\n",
    "`train.py` 文件中的 `main` 函数实现了模型训练的完整流程，包括数据准备、模型初始化、训练循环、损失计算、优化步骤以及模型评估。下面是详细的步骤分析：\n",
    "\n",
    "1. **配置加载**：\n",
    "   - 加载配置参数，这些参数在 `configurations.py` 文件中被定义。\n",
    "\n",
    "2. **数据加载器创建**：\n",
    "   - 使用 `create_dataloaders` 函数创建用于训练和测试的数据加载器。\n",
    "\n",
    "3. **词汇表加载**：\n",
    "   - 加载词汇表文件，这对于后续将文本编码和解码成数字是必要的。\n",
    "\n",
    "4. **模型初始化**：\n",
    "   - 实例化 `ARCTIC` 模型，传入必要的参数，如图像编码维度、词汇表、词嵌入维度等，并将模型转移到配置指定的设备上（如 GPU）。\n",
    "\n",
    "5. **优化器设置**：\n",
    "   - 调用 `get_optimizer` 函数为模型设置优化器，以用于训练中的参数更新。\n",
    "\n",
    "6. **损失函数定义**：\n",
    "   - 实例化 `PackedCrossEntropyLoss` 类，用于计算模型输出和目标序列之间的损失。\n",
    "\n",
    "7. **权重保存路径创建**：\n",
    "   - 创建用于保存训练过程中模型权重的目录。\n",
    "\n",
    "8. **训练循环**：\n",
    "   - 对于设定的训练轮次，执行以下操作：\n",
    "     - 将模型置于训练模式。\n",
    "     - 遍历训练数据加载器中的数据批次，对于每个批次：\n",
    "       - 将图像和文本数据移至配置指定的设备。\n",
    "       - 清空优化器状态。\n",
    "       - 通过模型传递图像和文本，获取输出和注意力权重。\n",
    "       - 计算损失，考虑到序列的实际长度。\n",
    "       - 执行反向传播和优化器步骤以更新权重。\n",
    "       - 定期打印损失信息。\n",
    "\n",
    "9. **模型评估**：\n",
    "   - 在每个训练轮次后，使用测试数据集评估模型性能，并打印 CIDEr 评分。\n",
    "\n",
    "10. **模型保存**：\n",
    "    - 如果当前模型性能好于之前的最佳性能，则保存模型权重（注释中提到的代码被注释掉了，但这是典型的做法）。\n",
    "    - 在训练完成后，保存最终的模型权重。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f997fcbcac13f22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from configurations import Config\n",
    "from models import ARCTIC, get_optimizer, PackedCrossEntropyLoss, evaluate_cider\n",
    "from datasets import create_dataloaders, ImageTextDataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 加载配置\n",
    "    config = Config()\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader, test_loader = create_dataloaders(config)\n",
    "\n",
    "    # 加载词汇表文件\n",
    "    with open('../data/output/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = ARCTIC(\n",
    "        image_code_dim=config.image_code_dim,\n",
    "        vocab=vocab,  # 传递词汇表字典\n",
    "        word_dim=config.word_dim,\n",
    "        attention_dim=config.attention_dim,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers\n",
    "    ).to(config.device)\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = get_optimizer(model, config)\n",
    "\n",
    "    # 损失函数\n",
    "    loss_fn = PackedCrossEntropyLoss().to(config.device)\n",
    "\n",
    "    # 创建保存权重的文件夹路径\n",
    "    weights_dir = os.path.join(config.output_folder, 'weights')\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "    best_val_score = float('-inf')  # 初始化最佳验证得分\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(config.num_epochs):\n",
    "        # 训练模型\n",
    "        model.train()\n",
    "        for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "            imgs, caps = imgs.to(config.device), caps.to(config.device)\n",
    "            caplens = caplens.cpu().to(torch.int64)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, alphas, _, _, _ = model(imgs, caps, caplens)\n",
    "\n",
    "            # 确保目标序列长度与模型输出匹配\n",
    "            targets = caps[:, 1:]  # 假设targets是captions去除第一个<start>标记后的部分\n",
    "            print(f\"Caplens: {caplens}\")\n",
    "            loss = loss_fn(outputs, targets, caplens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印/记录损失信息\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f'Epoch [{epoch + 1}/{config.num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # 在每个epoch结束时使用测试集评估模型\n",
    "        current_test_score = evaluate_cider(test_loader, model, config)\n",
    "        print(f\"Epoch {epoch}: Test score = {current_test_score}\")\n",
    "\n",
    "        # 如果当前得分比之前的最佳得分要好，则保存模型\n",
    "        # if current_val_score > best_val_score:\n",
    "        #     best_val_score = current_val_score\n",
    "        #     best_model_path = os.path.join(weights_dir, f'best_model_epoch_{epoch}.pth')\n",
    "        #     torch.save(model.state_dict(), best_model_path)\n",
    "        #     print(f\"Saved new best model to {best_model_path}\")\n",
    "\n",
    "    # 训练完成后的最终评估\n",
    "    final_test_score = evaluate_cider(test_loader, model, config)\n",
    "    print(f\"Final test score = {final_test_score}\")\n",
    "\n",
    "    # 训练完成后保存模型\n",
    "    final_model_path = os.path.join(weights_dir, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Saved final model to {final_model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c694538fb726ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
